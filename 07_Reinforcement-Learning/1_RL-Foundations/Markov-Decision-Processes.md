# Markov Decision Processes (MDPs)

> **The Math of Decision Making** - States, Transitions, and Rewards

---

## üé≤ The Markov Property

"The future depends only on the present, not the past."

$$ P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \dots) $$

If the state captures *everything* relevant about the world, history doesn't matter.
*Example:* In Chess, the current board configuration is all you need. You don't need to know *how* the pieces got there.

---

## üß© Components of an MDP

Defined as a tuple $(S, A, P, R, \gamma)$.

1.  **$S$ (State Space):** Set of all possible states.
2.  **$A$ (Action Space):** Set of all possible actions.
3.  **$P$ (Transition Probability):** Dynamics of the world.
    $$ P(s' | s, a) = \text{Probability of moving to } s' \text{ from } s \text{ taking } a $$
4.  **$R$ (Reward Function):**
    $$ R(s, a, s') = \text{Expected reward for transition} $$
5.  **$\gamma$ (Discount Factor):** $0 \le \gamma \le 1$.

---

## üó∫Ô∏è Policy ($\pi$)

The strategy of the agent. A mapping from State to Action.

1.  **Deterministic Policy:** $a = \pi(s)$.
2.  **Stochastic Policy:** $\pi(a|s) = P(A_t=a | S_t=s)$.

**Goal:** Find the Optimal Policy $\pi^*$ that maximizes expected return.

---

## üìâ Trajectory (Rollout)

A sequence of states and actions generated by a policy:
$$ \tau = (s_0, a_0, r_1, s_1, a_1, r_2, s_2, \dots) $$

The probability of a trajectory:
$$ P(\tau) = P(s_0) \prod_{t=0}^{T-1} \pi(a_t|s_t) P(s_{t+1}|s_t, a_t) $$

---

## üéì Interview Focus

1.  **Is Poker a Markov Process?**
    - No. The "State" (cards on table + your hand) is **Partially Observable** (you don't see opponent's hand).
    - It is a **POMDP** (Partially Observable MDP). To make it Markovian, the state must include the belief distribution over opponent hands.

2.  **What if the Transition Function $P$ is unknown?**
    - That's **Model-Free RL** (Q-Learning, PPO). The agent learns by trial and error.
    - If $P$ is known, it's **Planning** (Dynamic Programming).

3.  **Finite vs Infinite MDPs?**
    - **Episodic:** Ends in a terminal state (Game Over).
    - **Continuing:** Goes on forever (Stock Market).

---

**MDPs: The chessboard of mathematics!**
