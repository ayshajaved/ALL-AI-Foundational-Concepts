# Model-Based RL Intro

> **Planning in Imagination** - Learning the Rules of the World

---

## üó∫Ô∏è Model-Free vs Model-Based

- **Model-Free (DQN, PPO):** Learn $Q(s, a)$ or $\pi(a|s)$ directly from experience.
    - *Pros:* Simpler, unbiased.
    - *Cons:* Sample inefficient (needs millions of steps).
- **Model-Based:** Learn the **Model** of the environment first.
    - **Transition Model:** $s_{t+1} \approx f(s_t, a_t)$.
    - **Reward Model:** $r_{t+1} \approx g(s_t, a_t)$.
    - Then use **Planning** (e.g., MCTS, Trajectory Optimization) to find the best action.

---

## üß† The Advantage: Sample Efficiency

If you know the model, you can simulate 1,000,000 steps in your head for every 1 step you take in the real world.
**Analogy:**
- **Model-Free:** Learning to play chess by playing 10,000 games.
- **Model-Based:** Learning the rules of chess, then thinking 5 moves ahead before moving.

---

## üìâ The Challenge: Model Bias

If your learned model is slightly wrong, your plan will be very wrong.
**Compound Error:**
- Step 1 error: $\epsilon$.
- Step 2 error: $\epsilon + \delta$.
- Step 10 error: Huge.

The agent exploits the "delusions" of the model (finding a shortcut that doesn't exist in reality).

---

## üèóÔ∏è Model Predictive Control (MPC)

A common planning algorithm.
1.  **Observe** current state $s_t$.
2.  **Optimize** a sequence of actions $a_t, \dots, a_{t+H}$ using the learned model to maximize predicted reward.
3.  **Execute** only the *first* action $a_t$.
4.  **Repeat** (Re-plan) at $t+1$.

This "re-planning" corrects for model errors at every step.

---

## üéì Interview Focus

1.  **Why is Model-Based RL more sample efficient?**
    - Because the model generalizes. If you learn "Gravity pulls things down", you can predict the trajectory of *any* object without trying to throw every single one.

2.  **What is "Dyna"?**
    - An architecture that combines Model-Free RL (learning from real experience) with Model-Based Planning (learning from simulated experience generated by the model).

---

**Model-Based: Thinking before acting!**
