# AI Glossary: A-Z

> **Complete AI terminology reference** - From foundational concepts to cutting-edge techniques

---

## A

**Activation Function** - Non-linear function applied to neuron outputs in neural networks (e.g., ReLU, sigmoid, tanh, GELU)

**Actor-Critic** - RL algorithm combining value-based and policy-based methods (e.g., A2C, A3C, PPO, SAC)

**Adversarial Attack** - Intentional perturbation of inputs to fool ML models

**AGI (Artificial General Intelligence)** - Hypothetical AI with human-level intelligence across all domains

**Attention Mechanism** - Technique allowing models to focus on relevant parts of input (key innovation in Transformers)

**Autoencoder** - Neural network that learns compressed representations by reconstructing inputs

**AutoML** - Automated machine learning - automating model selection, hyperparameter tuning, and architecture search

---

## B

**Backpropagation** - Algorithm for computing gradients in neural networks using chain rule

**Bagging (Bootstrap Aggregating)** - Ensemble method training multiple models on random subsets of data

**Batch Normalization** - Technique normalizing layer inputs to stabilize and accelerate training

**Bayesian Inference** - Statistical method updating beliefs based on evidence using Bayes' theorem

**BERT (Bidirectional Encoder Representations from Transformers)** - Pre-trained language model using masked language modeling

**Bias-Variance Tradeoff** - Fundamental ML concept balancing model complexity and generalization

**Boosting** - Ensemble method sequentially training models to correct previous errors (e.g., XGBoost, AdaBoost)

---

## C

**Catastrophic Forgetting** - Problem where neural networks forget previous tasks when learning new ones

**Causal Inference** - Methods for determining cause-effect relationships from data

**CNN (Convolutional Neural Network)** - Neural network architecture for processing grid-like data (images, video)

**Contrastive Learning** - Self-supervised learning technique learning representations by comparing similar/dissimilar examples

**Cross-Entropy Loss** - Loss function measuring difference between predicted and true probability distributions

**CTC (Connectionist Temporal Classification)** - Loss function for sequence-to-sequence tasks without alignment

---

## D

**Data Augmentation** - Technique creating training variations to improve generalization

**Diffusion Models** - Generative models learning to denoise data (e.g., Stable Diffusion, DALL-E 2)

**Dimensionality Reduction** - Reducing number of features while preserving information (e.g., PCA, t-SNE, UMAP)

**Dropout** - Regularization technique randomly deactivating neurons during training

**DQN (Deep Q-Network)** - Deep RL algorithm combining Q-learning with neural networks

---

## E

**Embedding** - Dense vector representation of discrete objects (words, images, etc.)

**Ensemble Learning** - Combining multiple models for better performance

**Epoch** - One complete pass through entire training dataset

**Explainability (XAI)** - Making AI decisions interpretable and understandable

---

## F

**Feature Engineering** - Creating informative features from raw data

**Few-Shot Learning** - Learning from very few examples per class

**Fine-Tuning** - Adapting pre-trained model to specific task

**Foundation Model** - Large pre-trained model serving as base for many tasks (e.g., GPT-4, BERT)

---

## G

**GAN (Generative Adversarial Network)** - Generative model with generator and discriminator competing

**Gradient Descent** - Optimization algorithm minimizing loss by following negative gradient

**Graph Neural Network (GNN)** - Neural network operating on graph-structured data

**GPT (Generative Pre-trained Transformer)** - Autoregressive language model family

---

## H

**Hallucination** - When LLMs generate plausible but incorrect information

**Hyperparameter** - Model configuration set before training (learning rate, batch size, etc.)

**Hidden Markov Model (HMM)** - Probabilistic model for sequential data with hidden states

---

## I

**Imitation Learning** - Learning by mimicking expert demonstrations

**Inference** - Using trained model to make predictions

**Information Theory** - Mathematical framework for quantifying information (entropy, KL divergence)

---

## K

**K-Means** - Clustering algorithm partitioning data into K clusters

**KL Divergence (Kullback-Leibler)** - Measure of difference between probability distributions

**Knowledge Distillation** - Transferring knowledge from large model to smaller one

---

## L

**Latent Space** - Learned compressed representation space in models like VAEs

**Learning Rate** - Hyperparameter controlling step size in optimization

**LSTM (Long Short-Term Memory)** - RNN variant addressing vanishing gradient problem

**LLM (Large Language Model)** - Massive transformer model trained on text (GPT, LLaMA, Claude)

**LoRA (Low-Rank Adaptation)** - Parameter-efficient fine-tuning method

---

## M

**MDP (Markov Decision Process)** - Mathematical framework for RL problems

**Meta-Learning** - Learning to learn - adapting quickly to new tasks

**Mixture of Experts (MoE)** - Architecture with multiple specialized sub-networks

**Multi-Head Attention** - Attention mechanism with multiple parallel attention layers

---

## N

**NAS (Neural Architecture Search)** - Automated search for optimal neural network architectures

**NLP (Natural Language Processing)** - AI field focused on language understanding and generation

**Normalization** - Scaling features to similar ranges

---

## O

**Overfitting** - Model learning training data too well, failing to generalize

**Optimization** - Finding model parameters minimizing loss function

---

## P

**PAC Learning (Probably Approximately Correct)** - Theoretical framework for learning guarantees

**Perceptron** - Simplest neural network unit

**Policy** - In RL, mapping from states to actions

**Positional Encoding** - Adding position information to transformer inputs

**Prompt Engineering** - Crafting inputs to guide LLM outputs

**Pruning** - Removing unnecessary model parameters

---

## Q

**Q-Learning** - Value-based RL algorithm learning action-value function

**Quantization** - Reducing numerical precision to compress models

---

## R

**RAG (Retrieval Augmented Generation)** - Enhancing LLMs with external knowledge retrieval

**Recurrent Neural Network (RNN)** - Neural network for sequential data

**Regularization** - Techniques preventing overfitting (L1, L2, dropout)

**Reinforcement Learning (RL)** - Learning through interaction and rewards

**ResNet (Residual Network)** - CNN architecture with skip connections

**RLHF (Reinforcement Learning from Human Feedback)** - Training LLMs using human preferences

---

## S

**Self-Attention** - Attention mechanism where input attends to itself

**Self-Supervised Learning** - Learning from unlabeled data using pretext tasks

**Seq2Seq (Sequence-to-Sequence)** - Model mapping input sequence to output sequence

**SGD (Stochastic Gradient Descent)** - Gradient descent using mini-batches

**Softmax** - Function converting logits to probability distribution

**SVM (Support Vector Machine)** - Classification algorithm finding optimal separating hyperplane

---

## T

**Transfer Learning** - Applying knowledge from one task to another

**Transformer** - Architecture based on self-attention (foundation of modern NLP)

**TTS (Text-to-Speech)** - Converting text to spoken audio

---

## U

**Underfitting** - Model too simple to capture data patterns

**Unsupervised Learning** - Learning from unlabeled data

---

## V

**VAE (Variational Autoencoder)** - Generative model learning probabilistic latent representations

**Vanishing Gradient** - Problem where gradients become too small in deep networks

**VC Dimension** - Measure of model capacity in learning theory

**Vision Transformer (ViT)** - Applying transformer architecture to images

---

## W

**Weight Decay** - Regularization adding penalty to large weights

---

## Z

**Zero-Shot Learning** - Performing tasks without task-specific training examples

---

## Symbols & Acronyms

**α (alpha)** - Learning rate
**β (beta)** - Momentum parameter, regularization strength
**γ (gamma)** - Discount factor in RL
**λ (lambda)** - Regularization parameter
**η (eta)** - Learning rate

**AI** - Artificial Intelligence
**ANI** - Artificial Narrow Intelligence
**ASI** - Artificial Super Intelligence
**ASR** - Automatic Speech Recognition
**AUC** - Area Under Curve
**BLEU** - Bilingual Evaluation Understudy (translation metric)
**CLIP** - Contrastive Language-Image Pre-training
**CV** - Computer Vision
**DL** - Deep Learning
**ELU** - Exponential Linear Unit
**FID** - Fréchet Inception Distance
**GELU** - Gaussian Error Linear Unit
**GRU** - Gated Recurrent Unit
**IoU** - Intersection over Union
**MAE** - Mean Absolute Error
**MCTS** - Monte Carlo Tree Search
**MLE** - Maximum Likelihood Estimation
**MSE** - Mean Squared Error
**NER** - Named Entity Recognition
**PEFT** - Parameter-Efficient Fine-Tuning
**PPO** - Proximal Policy Optimization
**ReLU** - Rectified Linear Unit
**RMSE** - Root Mean Squared Error
**ROC** - Receiver Operating Characteristic
**ROUGE** - Recall-Oriented Understudy for Gisting Evaluation
**SAC** - Soft Actor-Critic
**SOTA** - State of the Art
**TD** - Temporal Difference
**VQA** - Visual Question Answering

---

**Total Terms:** 150+
**Coverage:** Complete AI terminology from classical to modern
**Use:** Quick reference, interview prep, concept lookup
