# Algorithm Complexity Reference

> **Time & Space Complexity Quick Reference** - Essential for interviews and system design

---

## ğŸ“Š Machine Learning Algorithms

### Supervised Learning

| Algorithm | Training Time | Prediction Time | Space | Notes |
|-----------|--------------|-----------------|-------|-------|
| **Linear Regression** | O(nÂ·dÂ²) or O(nÂ²Â·d) | O(d) | O(d) | Normal equation vs GD |
| **Logistic Regression** | O(nÂ·dÂ·i) | O(d) | O(d) | i = iterations |
| **Decision Tree** | O(nÂ·dÂ·log n) | O(log n) | O(n) | Balanced tree |
| **Random Forest** | O(nÂ·dÂ·log nÂ·k) | O(kÂ·log n) | O(kÂ·n) | k = trees |
| **XGBoost/Gradient Boosting** | O(nÂ·dÂ·k) | O(kÂ·log n) | O(kÂ·n) | k = trees |
| **SVM (Linear)** | O(nÂ·d) | O(d) | O(d) | Linear kernel |
| **SVM (RBF)** | O(nÂ²Â·d) to O(nÂ³) | O(sÂ·d) | O(sÂ·d) | s = support vectors |
| **KNN** | O(1) | O(nÂ·d) | O(nÂ·d) | Lazy learning |
| **Naive Bayes** | O(nÂ·d) | O(cÂ·d) | O(cÂ·d) | c = classes |

### Unsupervised Learning

| Algorithm | Time Complexity | Space | Notes |
|-----------|----------------|-------|-------|
| **K-Means** | O(nÂ·kÂ·dÂ·i) | O(nÂ·d) | i = iterations, k = clusters |
| **Hierarchical Clustering** | O(nÂ²Â·log n) | O(nÂ²) | Agglomerative |
| **DBSCAN** | O(nÂ·log n) | O(n) | With spatial index |
| **GMM (EM)** | O(nÂ·kÂ·dÂ·i) | O(nÂ·d) | Similar to K-Means |
| **PCA** | O(min(nÂ²Â·d, nÂ·dÂ²)) | O(dÂ²) | SVD-based |
| **t-SNE** | O(nÂ²) | O(n) | Very slow for large n |
| **UMAP** | O(n^1.14) | O(n) | Faster than t-SNE |

---

## ğŸ”¥ Deep Learning Operations

### Basic Operations

| Operation | Time | Space | Notes |
|-----------|------|-------|-------|
| **Matrix Multiplication** (mÃ—n) Â· (nÃ—p) | O(mÂ·nÂ·p) | O(mÂ·p) | Output size |
| **Convolution** (HÃ—WÃ—C) * (KÃ—KÃ—C) | O(HÂ·WÂ·CÂ·KÂ²Â·F) | O(HÂ·WÂ·F) | F = filters |
| **Batch Normalization** | O(nÂ·d) | O(d) | n = batch size |
| **Dropout** | O(n) | O(n) | Per neuron |
| **Softmax** | O(n) | O(n) | n = classes |
| **Attention** (seq_len = n, d_model = d) | O(nÂ²Â·d) | O(nÂ²) | Quadratic in sequence |

### Neural Network Layers

| Layer Type | Forward Pass | Backward Pass | Parameters | Notes |
|------------|-------------|---------------|------------|-------|
| **Fully Connected** | O(nÂ·m) | O(nÂ·m) | O(nÂ·m) | n=input, m=output |
| **Conv2D** | O(HÂ·WÂ·CÂ·KÂ²Â·F) | O(HÂ·WÂ·CÂ·KÂ²Â·F) | O(KÂ²Â·CÂ·F) | K=kernel, F=filters |
| **MaxPool** | O(HÂ·WÂ·C) | O(HÂ·WÂ·C) | 0 | No parameters |
| **LSTM** | O(4Â·hÂ²) | O(4Â·hÂ²) | O(4Â·hÂ²) | h = hidden size |
| **GRU** | O(3Â·hÂ²) | O(3Â·hÂ²) | O(3Â·hÂ²) | Fewer params than LSTM |
| **Self-Attention** | O(nÂ²Â·d) | O(nÂ²Â·d) | O(dÂ²) | n=seq_len, d=d_model |
| **Multi-Head Attention** | O(nÂ²Â·dÂ·h) | O(nÂ²Â·dÂ·h) | O(dÂ²Â·h) | h = heads |

### Training Complexity

| Model | Training (per epoch) | Inference | Parameters |
|-------|---------------------|-----------|------------|
| **MLP** (L layers, h units) | O(LÂ·hÂ²Â·n) | O(LÂ·hÂ²) | O(LÂ·hÂ²) |
| **CNN** (L layers) | O(LÂ·HÂ·WÂ·CÂ·KÂ²Â·FÂ·n) | O(LÂ·HÂ·WÂ·CÂ·KÂ²Â·F) | O(LÂ·KÂ²Â·CÂ·F) |
| **RNN** (T timesteps) | O(TÂ·hÂ²Â·n) | O(TÂ·hÂ²) | O(hÂ²) |
| **Transformer** (L layers) | O(LÂ·nÂ²Â·dÂ·b) | O(LÂ·nÂ²Â·d) | O(LÂ·dÂ²) |

---

## ğŸ’¬ NLP Algorithms

| Algorithm | Time | Space | Notes |
|-----------|------|-------|-------|
| **Word2Vec (Skip-gram)** | O(VÂ·dÂ·wÂ·i) | O(VÂ·d) | V=vocab, w=window, i=iters |
| **GloVe** | O(CÂ·i) | O(VÂ·d) | C = co-occurrences |
| **BERT Inference** | O(LÂ·nÂ²Â·d) | O(nÂ·d) | L=layers, n=seq_len |
| **GPT Inference** | O(LÂ·nÂ²Â·d) | O(nÂ·d) | Autoregressive |
| **Beam Search** | O(bÂ·nÂ·V) | O(bÂ·n) | b=beam width, V=vocab |

---

## ğŸ‘ï¸ Computer Vision

| Algorithm | Time | Space | Notes |
|-----------|------|-------|-------|
| **SIFT** | O(nÂ·log n) | O(n) | n = pixels |
| **HOG** | O(n) | O(n) | Linear in pixels |
| **R-CNN** | O(rÂ·c) | O(r) | r=regions, c=CNN cost |
| **Fast R-CNN** | O(r + c) | O(r) | Shared CNN |
| **Faster R-CNN** | O(c + n) | O(n) | RPN + detection |
| **YOLO** | O(SÂ²Â·(BÂ·5+C)) | O(SÂ²Â·B) | S=grid, B=boxes, C=classes |
| **Mask R-CNN** | O(c + rÂ·m) | O(rÂ·m) | m=mask resolution |

---

## ğŸ® Reinforcement Learning

| Algorithm | Time (per step) | Space | Notes |
|-----------|----------------|-------|-------|
| **Q-Learning** | O(1) | O(|S|Â·|A|) | Tabular |
| **SARSA** | O(1) | O(|S|Â·|A|) | Tabular |
| **DQN** | O(d) | O(|S|Â·d) | d = network size |
| **Policy Gradient** | O(d) | O(d) | Network forward pass |
| **A3C** | O(dÂ·w) | O(dÂ·w) | w = workers |
| **PPO** | O(dÂ·b) | O(dÂ·b) | b = batch size |

---

## ğŸ“Š Data Structures

| Structure | Access | Search | Insert | Delete | Space |
|-----------|--------|--------|--------|--------|-------|
| **Array** | O(1) | O(n) | O(n) | O(n) | O(n) |
| **Linked List** | O(n) | O(n) | O(1) | O(1) | O(n) |
| **Hash Table** | O(1)* | O(1)* | O(1)* | O(1)* | O(n) |
| **Binary Search Tree** | O(log n)* | O(log n)* | O(log n)* | O(log n)* | O(n) |
| **Heap** | O(1) | O(n) | O(log n) | O(log n) | O(n) |
| **Trie** | O(m) | O(m) | O(m) | O(m) | O(ALPHABET_SIZEÂ·NÂ·M) |

*Average case; worst case may differ

---

## ğŸ” Search & Sort Algorithms

### Sorting

| Algorithm | Best | Average | Worst | Space | Stable |
|-----------|------|---------|-------|-------|--------|
| **Bubble Sort** | O(n) | O(nÂ²) | O(nÂ²) | O(1) | Yes |
| **Selection Sort** | O(nÂ²) | O(nÂ²) | O(nÂ²) | O(1) | No |
| **Insertion Sort** | O(n) | O(nÂ²) | O(nÂ²) | O(1) | Yes |
| **Merge Sort** | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes |
| **Quick Sort** | O(n log n) | O(n log n) | O(nÂ²) | O(log n) | No |
| **Heap Sort** | O(n log n) | O(n log n) | O(n log n) | O(1) | No |
| **Radix Sort** | O(dÂ·n) | O(dÂ·n) | O(dÂ·n) | O(n+k) | Yes |

### Searching

| Algorithm | Time | Space | Notes |
|-----------|------|-------|-------|
| **Linear Search** | O(n) | O(1) | Unsorted array |
| **Binary Search** | O(log n) | O(1) | Sorted array |
| **BFS** | O(V+E) | O(V) | Graph traversal |
| **DFS** | O(V+E) | O(V) | Graph traversal |
| **A*** | O(b^d) | O(b^d) | b=branching, d=depth |
| **Dijkstra** | O((V+E) log V) | O(V) | With min-heap |

---

## ğŸ¯ Interview Quick Reference

### Most Common Complexities

**O(1)** - Constant
- Array access
- Hash table operations (average)
- Stack/Queue operations

**O(log n)** - Logarithmic
- Binary search
- Balanced tree operations
- Heap operations

**O(n)** - Linear
- Array traversal
- Linear search
- Most single-pass algorithms

**O(n log n)** - Linearithmic
- Efficient sorting (Merge, Heap, Quick)
- Many divide-and-conquer algorithms

**O(nÂ²)** - Quadratic
- Nested loops
- Simple sorting (Bubble, Selection, Insertion)
- Naive string matching

**O(2^n)** - Exponential
- Recursive Fibonacci (naive)
- Subset generation
- Brute force solutions

**O(n!)** - Factorial
- Permutation generation
- Traveling salesman (brute force)

---

## ğŸ’¡ Optimization Tips

### When to Optimize
1. **O(nÂ²) â†’ O(n log n)**: Use sorting or divide-and-conquer
2. **O(nÂ²) â†’ O(n)**: Use hash table or two pointers
3. **O(2^n) â†’ O(nÂ²)**: Use dynamic programming
4. **O(n) â†’ O(log n)**: Use binary search (if sorted)

### Space-Time Tradeoffs
- **Memoization**: Trade O(n) space for better time
- **Hash tables**: O(n) space for O(1) lookup
- **Preprocessing**: Upfront cost for faster queries

### ML-Specific Optimizations
- **Mini-batch GD**: Balance between SGD (O(d)) and Batch GD (O(nÂ·d))
- **Approximate NN**: Use LSH or ANNOY for faster KNN
- **Model compression**: Reduce inference time/space
- **Distributed training**: Parallelize across GPUs

---

## ğŸ“ Interview Strategy

### When Asked About Complexity
1. **State assumptions** (input size, data structure)
2. **Analyze loops** (nested = multiply)
3. **Consider best/average/worst** cases
4. **Mention space complexity** too
5. **Suggest optimizations** if asked

### Red Flags in Interviews
- âŒ O(nÂ²) when O(n log n) possible
- âŒ O(2^n) when DP can solve in O(nÂ²)
- âŒ Not considering space complexity
- âŒ Not knowing standard algorithm complexities

---

**Master these complexities for technical interviews!**
