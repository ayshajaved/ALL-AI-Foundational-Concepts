# Support Vector Machines (SVM)

> **Maximum margin classification** - Finding the optimal separating hyperplane

---

## ðŸŽ¯ Linear SVM

### Idea
Find hyperplane that maximizes margin between classes

```
Decision boundary: wáµ€x + b = 0
Margin: 2/||w||

Maximize margin = Minimize ||w||Â²
```

### Hard Margin SVM

**For linearly separable data:**

```
minimize Â½||w||Â²
subject to yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1, âˆ€i
```

```python
from sklearn.svm import SVC
from sklearn.datasets import make_blobs

# Linearly separable data
X, y = make_blobs(n_samples=100, centers=2, random_state=42)
y = 2*y - 1  # Convert to {-1, 1}

# Train
model = SVC(kernel='linear', C=1e10)  # Large C â‰ˆ hard margin
model.fit(X, y)

# Support vectors
print(f"Support vectors: {model.support_vectors_}")
print(f"Number: {len(model.support_vectors_)}")
```

---

## ðŸ“Š Soft Margin SVM

**For non-separable data:**

```
minimize Â½||w||Â² + C Î£áµ¢ Î¾áµ¢
subject to yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1 - Î¾áµ¢
           Î¾áµ¢ â‰¥ 0

Î¾áµ¢: slack variables (allow misclassification)
C: regularization parameter
```

```python
# Soft margin
model_soft = SVC(kernel='linear', C=1.0)
model_soft.fit(X, y)

# C controls trade-off
C_values = [0.01, 0.1, 1, 10, 100]
for C in C_values:
    model = SVC(kernel='linear', C=C)
    model.fit(X_train, y_train)
    print(f"C={C}: {len(model.support_vectors_)} support vectors")
```

---

## ðŸŽ¯ Kernel Trick

### Idea
Map to higher dimension where data is separable

```
Ï†: X â†’ H (feature map)
K(x, x') = Ï†(x)áµ€Ï†(x') (kernel function)

Never compute Ï† explicitly!
```

### Common Kernels

**1. Linear**
```
K(x, x') = xáµ€x'
```

**2. Polynomial**
```
K(x, x') = (Î³xáµ€x' + r)áµˆ

d: degree
```

**3. RBF (Gaussian)**
```
K(x, x') = exp(-Î³||x - x'||Â²)

Î³: kernel coefficient
```

**4. Sigmoid**
```
K(x, x') = tanh(Î³xáµ€x' + r)
```

```python
# RBF kernel
model_rbf = SVC(kernel='rbf', gamma='scale', C=1.0)
model_rbf.fit(X, y)

# Polynomial kernel
model_poly = SVC(kernel='poly', degree=3, C=1.0)
model_poly.fit(X, y)

# Custom kernel
def custom_kernel(X, Y):
    return np.dot(X, Y.T)

model_custom = SVC(kernel=custom_kernel)
model_custom.fit(X, y)
```

---

## ðŸ“ˆ Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
    'kernel': ['rbf', 'poly']
}

grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)

print(f"Best params: {grid.best_params_}")
print(f"Best score: {grid.best_score_:.4f}")

# Use best model
best_model = grid.best_estimator_
```

---

## ðŸŽ¯ Multi-class SVM

### One-vs-One (OvO)
```
Train K(K-1)/2 binary classifiers
Predict by voting
```

### One-vs-Rest (OvR)
```
Train K binary classifiers
Predict class with highest score
```

```python
# Multi-class (OvO by default)
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target

model_multi = SVC(kernel='rbf', decision_function_shape='ovo')
model_multi.fit(X, y)

# OvR
model_ovr = SVC(kernel='rbf', decision_function_shape='ovr')
model_ovr.fit(X, y)
```

---

## ðŸ“Š SVM Regression (SVR)

```python
from sklearn.svm import SVR

# Regression
X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)
y_reg = np.sin(X_reg).ravel() + np.random.randn(100) * 0.1

model_svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
model_svr.fit(X_reg, y_reg)

# Predict
X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = model_svr.predict(X_test)
```

---

## ðŸŽ“ Interview Focus

### Key Questions

1. **What is SVM?**
   - Maximum margin classifier
   - Finds optimal hyperplane
   - Uses support vectors

2. **Kernel trick?**
   - Map to higher dimension
   - Never compute mapping explicitly
   - Use kernel function K(x,x')

3. **C parameter?**
   - Regularization strength
   - Large C: hard margin (less regularization)
   - Small C: soft margin (more regularization)

4. **Î³ in RBF kernel?**
   - Defines influence of single training example
   - Large Î³: close influence (overfitting)
   - Small Î³: far influence (underfitting)

5. **SVM vs Logistic Regression?**
   - SVM: maximum margin
   - LR: probabilistic
   - SVM better for small datasets
   - LR faster for large datasets

---

## ðŸ“š References

- **Papers:** "A Tutorial on Support Vector Machines" - Burges

---

**SVM: powerful, kernel-based, margin-maximizing!**
