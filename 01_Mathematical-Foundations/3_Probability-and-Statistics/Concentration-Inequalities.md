# Concentration Inequalities

> **Bounding tail probabilities** - Essential for learning theory

---

## üéØ Why Concentration Inequalities?

**Problem:** How much does sample mean deviate from true mean?

**Answer:** Concentration inequalities provide probabilistic bounds

**Applications:**
- Generalization bounds in ML
- PAC learning theory
- Algorithm analysis

---

## üìä Markov's Inequality

### Statement
For non-negative random variable X:
```
P(X ‚â• a) ‚â§ E[X] / a
```

**Example:**
```python
# If average height is 170cm
# P(height ‚â• 340cm) ‚â§ 170/340 = 0.5
# Weak bound, but always valid!
```

---

## üéØ Chebyshev's Inequality

### Statement
```
P(|X - Œº| ‚â• kœÉ) ‚â§ 1/k¬≤

where Œº = E[X], œÉ¬≤ = Var(X)
```

**Equivalent form:**
```
P(|X - Œº| ‚â• Œµ) ‚â§ œÉ¬≤/Œµ¬≤
```

**Example:**
```python
import numpy as np

# Generate data
data = np.random.randn(10000)
mu = np.mean(data)
sigma = np.std(data)

# Chebyshev bound for k=2
k = 2
empirical_prob = np.mean(np.abs(data - mu) >= k*sigma)
chebyshev_bound = 1/k**2

print(f"Empirical: {empirical_prob:.4f}")
print(f"Chebyshev bound: {chebyshev_bound:.4f}")
# Empirical ‚â§ Bound (usually much smaller)
```

---

## üî• Hoeffding's Inequality

### Statement
For independent bounded r.v. X‚ÇÅ,...,X‚Çô ‚àà [a,b]:
```
P(|XÃÑ - Œº| ‚â• Œµ) ‚â§ 2exp(-2nŒµ¬≤/(b-a)¬≤)

where XÃÑ = (X‚ÇÅ + ... + X‚Çô)/n
```

**Key insight:** Exponential decay in n!

**Example:**
```python
def hoeffding_bound(n, epsilon, a=0, b=1):
    """Hoeffding bound for sample mean"""
    return 2 * np.exp(-2 * n * epsilon**2 / (b - a)**2)

# How many samples for Œµ=0.1 with 95% confidence?
epsilon = 0.1
delta = 0.05  # 1 - confidence

n_required = int(np.ceil((b-a)**2 * np.log(2/delta) / (2*epsilon**2)))
print(f"Samples needed: {n_required}")
```

---

## üìà Chernoff Bound

### Statement
For sum of independent Bernoulli r.v.:
```
X = X‚ÇÅ + ... + X‚Çô, X·µ¢ ~ Bernoulli(p)

P(X ‚â• (1+Œ¥)np) ‚â§ exp(-Œ¥¬≤np/3)  for Œ¥ ‚àà [0,1]
P(X ‚â§ (1-Œ¥)np) ‚â§ exp(-Œ¥¬≤np/2)  for Œ¥ ‚àà [0,1]
```

**Tighter than Hoeffding for Bernoulli!**

---

## üéØ Applications in ML

### 1. Sample Complexity

**Question:** How many samples to learn with error ‚â§ Œµ?

**Answer (via Hoeffding):**
```
n ‚â• (1/(2Œµ¬≤)) log(2/Œ¥)
```

```python
def sample_complexity(epsilon, delta):
    """Samples needed for Œµ-accurate estimate"""
    return int(np.ceil(np.log(2/delta) / (2*epsilon**2)))

# For Œµ=0.01, Œ¥=0.05
n = sample_complexity(0.01, 0.05)
print(f"Need {n} samples")  # ~18,445
```

### 2. Generalization Bound

**Empirical risk:** RÃÇ(h) = (1/n)Œ£·µ¢ L(h(x·µ¢), y·µ¢)
**True risk:** R(h) = E[L(h(x), y)]

**Hoeffding bound:**
```
P(|R(h) - RÃÇ(h)| ‚â• Œµ) ‚â§ 2exp(-2nŒµ¬≤)
```

With probability ‚â• 1-Œ¥:
```
R(h) ‚â§ RÃÇ(h) + ‚àö(log(2/Œ¥)/(2n))
```

```python
def generalization_bound(n, delta):
    """Upper bound on generalization error"""
    return np.sqrt(np.log(2/delta) / (2*n))

# For n=1000 samples
bound = generalization_bound(1000, 0.05)
print(f"Generalization error ‚â§ train error + {bound:.4f}")
```

### 3. PAC Learning

**Probably Approximately Correct (PAC):**
```
P(R(h) ‚â§ Œµ) ‚â• 1 - Œ¥
```

**Sample complexity:**
```
n = O((1/Œµ¬≤)log(|H|/Œ¥))

where |H| = hypothesis class size
```

---

## üéì Interview Focus

### Key Questions

1. **What is Hoeffding's inequality?**
   - Bounds deviation of sample mean
   - Exponential decay in n
   - Doesn't need variance!

2. **Why concentration inequalities matter?**
   - Generalization bounds
   - Sample complexity
   - PAC learning theory

3. **Hoeffding vs Chebyshev?**
   - Hoeffding: exponential, needs bounded r.v.
   - Chebyshev: polynomial, only needs variance

4. **Sample complexity formula?**
   - n = O(1/Œµ¬≤ log(1/Œ¥))
   - Quadratic in 1/Œµ
   - Logarithmic in 1/Œ¥

---

## üìö References

- **Books:** "Concentration Inequalities" - Boucheron et al.
- **Papers:** "A Few Useful Things to Know about Machine Learning" - Domingos

---

**Concentration inequalities: theoretical foundation of ML!**
