# Probability Fundamentals

> **Foundation of uncertainty in AI** - Understanding randomness and probability

---

## ðŸŽ² Basic Probability

### Sample Space and Events

**Sample Space (Î©):** Set of all possible outcomes
**Event (A):** Subset of sample space

**Example:**
```
Coin flip: Î© = {H, T}
Die roll: Î© = {1, 2, 3, 4, 5, 6}
Event A = {2, 4, 6} (even numbers)
```

### Probability Axioms

**Kolmogorov Axioms:**
```
1. P(A) â‰¥ 0 for all events A
2. P(Î©) = 1
3. P(A âˆª B) = P(A) + P(B) if A âˆ© B = âˆ…
```

---

## ðŸ“Š Probability Rules

### Addition Rule
```
P(A âˆª B) = P(A) + P(B) - P(A âˆ© B)
```

### Multiplication Rule
```
P(A âˆ© B) = P(A|B)P(B) = P(B|A)P(A)
```

### Conditional Probability
```
P(A|B) = P(A âˆ© B) / P(B)
```

### Law of Total Probability
```
P(A) = Î£áµ¢ P(A|Báµ¢)P(Báµ¢)
where {Báµ¢} partition the sample space
```

### Bayes' Theorem
```
P(A|B) = P(B|A)P(A) / P(B)

Posterior = (Likelihood Ã— Prior) / Evidence
```

---

## ðŸŽ¯ Random Variables

### Definition
A **random variable** X maps outcomes to real numbers:
```
X: Î© â†’ â„
```

### Types

**Discrete:** Countable outcomes
```
X âˆˆ {xâ‚, xâ‚‚, ..., xâ‚™}
Example: Number of heads in coin flips
```

**Continuous:** Uncountable outcomes
```
X âˆˆ â„ or interval
Example: Height, temperature
```

---

## ðŸ“ˆ Probability Distributions

### Probability Mass Function (PMF)
For discrete X:
```
pâ‚“(x) = P(X = x)

Properties:
- pâ‚“(x) â‰¥ 0
- Î£â‚“ pâ‚“(x) = 1
```

### Probability Density Function (PDF)
For continuous X:
```
fâ‚“(x) such that P(a â‰¤ X â‰¤ b) = âˆ«â‚áµ‡ fâ‚“(x)dx

Properties:
- fâ‚“(x) â‰¥ 0
- âˆ«â‚‹âˆž^âˆž fâ‚“(x)dx = 1
```

### Cumulative Distribution Function (CDF)
```
Fâ‚“(x) = P(X â‰¤ x)

For discrete: Fâ‚“(x) = Î£â‚œâ‰¤â‚“ pâ‚“(t)
For continuous: Fâ‚“(x) = âˆ«â‚‹âˆžË£ fâ‚“(t)dt
```

---

## ðŸ“Š Common Distributions

### Bernoulli
```
X âˆˆ {0, 1}
P(X = 1) = p
P(X = 0) = 1 - p

E[X] = p
Var(X) = p(1-p)
```

### Binomial
```
X = number of successes in n trials
P(X = k) = C(n,k)páµ(1-p)â¿â»áµ

E[X] = np
Var(X) = np(1-p)
```

### Poisson
```
X = number of events in interval
P(X = k) = (Î»áµeâ»Î») / k!

E[X] = Î»
Var(X) = Î»
```

### Uniform (Continuous)
```
X ~ U(a, b)
f(x) = 1/(b-a) for x âˆˆ [a,b]

E[X] = (a+b)/2
Var(X) = (b-a)Â²/12
```

### Gaussian (Normal)
```
X ~ N(Î¼, ÏƒÂ²)
f(x) = (1/âˆš(2Ï€ÏƒÂ²))exp(-(x-Î¼)Â²/(2ÏƒÂ²))

E[X] = Î¼
Var(X) = ÏƒÂ²
```

### Exponential
```
X ~ Exp(Î»)
f(x) = Î»eâ»Î»Ë£ for x â‰¥ 0

E[X] = 1/Î»
Var(X) = 1/Î»Â²
```

---

## ðŸ’» Practical Implementation

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Bernoulli
p = 0.7
bernoulli = stats.bernoulli(p)
print(f"P(X=1) = {bernoulli.pmf(1)}")

# Binomial
n, p = 10, 0.5
binomial = stats.binom(n, p)
print(f"P(X=5) = {binomial.pmf(5)}")
print(f"E[X] = {binomial.mean()}")

# Poisson
lambda_param = 3
poisson = stats.poisson(lambda_param)
print(f"P(X=2) = {poisson.pmf(2)}")

# Normal
mu, sigma = 0, 1
normal = stats.norm(mu, sigma)
print(f"P(X â‰¤ 0) = {normal.cdf(0)}")

# Generate samples
samples = normal.rvs(size=1000)
print(f"Sample mean: {samples.mean():.3f}")
print(f"Sample std: {samples.std():.3f}")

# Plot PDF
x = np.linspace(-4, 4, 100)
plt.plot(x, normal.pdf(x))
plt.title('Standard Normal Distribution')
plt.show()
```

---

## ðŸŽ¯ Expected Value and Variance

### Expected Value (Mean)
```
Discrete: E[X] = Î£â‚“ xÂ·P(X=x)
Continuous: E[X] = âˆ«â‚‹âˆž^âˆž xÂ·f(x)dx
```

**Properties:**
```
E[aX + b] = aE[X] + b
E[X + Y] = E[X] + E[Y]
E[XY] = E[X]E[Y] if X,Y independent
```

### Variance
```
Var(X) = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²
```

**Properties:**
```
Var(aX + b) = aÂ²Var(X)
Var(X + Y) = Var(X) + Var(Y) if X,Y independent
```

### Standard Deviation
```
Ïƒ = âˆšVar(X)
```

---

## ðŸ”— Joint Distributions

### Joint PMF/PDF
```
Discrete: p(x,y) = P(X=x, Y=y)
Continuous: f(x,y)
```

### Marginal Distributions
```
Discrete: pâ‚“(x) = Î£áµ§ p(x,y)
Continuous: fâ‚“(x) = âˆ«â‚‹âˆž^âˆž f(x,y)dy
```

### Independence
```
X and Y independent âŸº p(x,y) = pâ‚“(x)páµ§(y)
```

### Covariance
```
Cov(X,Y) = E[(X-E[X])(Y-E[Y])]
         = E[XY] - E[X]E[Y]
```

### Correlation
```
Ï(X,Y) = Cov(X,Y) / (Ïƒâ‚“Ïƒáµ§)
Ï âˆˆ [-1, 1]
```

---

## ðŸŽ“ Interview Focus

### Key Questions

1. **What is Bayes' theorem?**
   - P(A|B) = P(B|A)P(A)/P(B)
   - Updates beliefs with evidence
   - Foundation of Bayesian ML

2. **Difference between PMF and PDF?**
   - PMF: discrete (probabilities)
   - PDF: continuous (densities)
   - PDF can be > 1!

3. **What is independence?**
   - P(Aâˆ©B) = P(A)P(B)
   - Knowing B doesn't change P(A)
   - Critical assumption in ML

4. **Expected value vs mean?**
   - E[X]: theoretical average
   - Mean: sample average
   - Law of large numbers connects them

5. **Why is normal distribution important?**
   - Central limit theorem
   - Many natural phenomena
   - Mathematical convenience

### Must-Know Formulas

```
Bayes: P(A|B) = P(B|A)P(A)/P(B)
E[X] = Î£xÂ·P(X=x)
Var(X) = E[XÂ²] - (E[X])Â²
Cov(X,Y) = E[XY] - E[X]E[Y]
```

---

## ðŸ“š References

- **Books:** "Probability and Statistics" - DeGroot & Schervish
- **Online:** Khan Academy, 3Blue1Brown

---

**Probability is the language of uncertainty in AI!**
