# Integration and Applications

> **The reverse of differentiation** - Accumulation, area, and probabilistic reasoning

---

## ğŸ“ Integration Basics

### Indefinite Integral (Antiderivative)

```
âˆ«f(x)dx = F(x) + C

where F'(x) = f(x)
```

### Definite Integral

```
âˆ«â‚áµ‡ f(x)dx = F(b) - F(a)
```

**Geometric Interpretation:** Area under curve from a to b

**Historical Note:** Developed independently by Newton and Leibniz (1670s), revolutionized mathematics by connecting differentiation and integration.

---

## ğŸ”¢ Integration Rules

### Power Rule
```
âˆ«xâ¿dx = xâ¿âºÂ¹/(n+1) + C  (n â‰  -1)
```

### Common Integrals
```
âˆ«eË£dx = eË£ + C
âˆ«(1/x)dx = ln|x| + C
âˆ«sin(x)dx = -cos(x) + C
âˆ«cos(x)dx = sin(x) + C
âˆ«1/(1+xÂ²)dx = arctan(x) + C
```

### Integration by Parts
```
âˆ«u dv = uv - âˆ«v du

Mnemonic: LIATE (choose u in this order)
L: Logarithmic
I: Inverse trig
A: Algebraic
T: Trigonometric
E: Exponential
```

### Substitution
```
âˆ«f(g(x))g'(x)dx = âˆ«f(u)du  where u = g(x)
```

**Example:**
```python
# âˆ«2xÂ·e^(xÂ²)dx
# Let u = xÂ², du = 2x dx
# âˆ«e^u du = e^u + C = e^(xÂ²) + C
```

---

## ğŸ¯ Numerical Integration

### Riemann Sum
```
âˆ«â‚áµ‡ f(x)dx â‰ˆ Î£áµ¢ f(xáµ¢)Î”x

Error: O(Î”x) = O((b-a)/n)
```

### Trapezoidal Rule
```
âˆ«â‚áµ‡ f(x)dx â‰ˆ (b-a)/2n Â· [f(xâ‚€) + 2f(xâ‚) + ... + 2f(xâ‚™â‚‹â‚) + f(xâ‚™)]

Error: O((b-a)Â³/nÂ²)
```

### Simpson's Rule
```
âˆ«â‚áµ‡ f(x)dx â‰ˆ (b-a)/3n Â· [f(xâ‚€) + 4f(xâ‚) + 2f(xâ‚‚) + 4f(xâ‚ƒ) + ... + f(xâ‚™)]

Error: O((b-a)âµ/nâ´)
Much better than trapezoidal!
```

### Gaussian Quadrature
```
âˆ«â‚‹â‚Â¹ f(x)dx â‰ˆ Î£áµ¢ wáµ¢f(xáµ¢)

Optimal choice of points xáµ¢ and weights wáµ¢
Exact for polynomials up to degree 2n-1
```

---

## ğŸ’» Practical Implementation

```python
import numpy as np
from scipy import integrate
import matplotlib.pyplot as plt

# Numerical integration
def riemann_sum(f, a, b, n=1000):
    """Riemann sum approximation"""
    x = np.linspace(a, b, n)
    dx = (b - a) / n
    return np.sum(f(x) * dx)

def trapezoidal(f, a, b, n=1000):
    """Trapezoidal rule"""
    x = np.linspace(a, b, n+1)
    y = f(x)
    dx = (b - a) / n
    return dx * (y[0]/2 + np.sum(y[1:-1]) + y[-1]/2)

def simpsons(f, a, b, n=1000):
    """Simpson's rule (n must be even)"""
    if n % 2 == 1:
        n += 1
    x = np.linspace(a, b, n+1)
    y = f(x)
    dx = (b - a) / n
    return dx/3 * (y[0] + 4*np.sum(y[1:-1:2]) + 2*np.sum(y[2:-1:2]) + y[-1])

# Using SciPy (adaptive quadrature)
f = lambda x: x**2
result, error = integrate.quad(f, 0, 1)
print(f"Integral: {result:.6f}")  # Should be 1/3
print(f"Error estimate: {error:.2e}")

# Compare methods
true_value = 1/3
for n in [10, 100, 1000]:
    riemann = riemann_sum(f, 0, 1, n)
    trap = trapezoidal(f, 0, 1, n)
    simp = simpsons(f, 0, 1, n)
    print(f"n={n}:")
    print(f"  Riemann error: {abs(riemann - true_value):.2e}")
    print(f"  Trapezoid error: {abs(trap - true_value):.2e}")
    print(f"  Simpson error: {abs(simp - true_value):.2e}")
```

---

## ğŸ² Monte Carlo Integration

### Basic Monte Carlo
```python
def monte_carlo_integrate(f, a, b, n=10000):
    """Monte Carlo integration"""
    x = np.random.uniform(a, b, n)
    return (b - a) * np.mean(f(x))

# Example
f = lambda x: np.exp(-x**2)
result = monte_carlo_integrate(f, 0, 1, 100000)
print(f"MC estimate: {result:.6f}")

# True value
true_val, _ = integrate.quad(f, 0, 1)
print(f"True value: {true_val:.6f}")
```

### Importance Sampling
```python
def importance_sampling(f, proposal_dist, proposal_pdf, n=10000):
    """Importance sampling for integration"""
    # Sample from proposal distribution
    samples = proposal_dist(n)
    
    # Compute weights
    weights = f(samples) / proposal_pdf(samples)
    
    return np.mean(weights)

# Example: âˆ«e^(-xÂ²)dx from 0 to âˆ
# Use exponential proposal
proposal_dist = lambda n: np.random.exponential(1, n)
proposal_pdf = lambda x: np.exp(-x)
f = lambda x: np.exp(-x**2)

result = importance_sampling(f, proposal_dist, proposal_pdf, 100000)
print(f"Importance sampling: {result:.6f}")
```

### Multidimensional Integration
```python
# Monte Carlo shines in high dimensions!
def mc_integrate_nd(f, bounds, n=100000):
    """Monte Carlo integration in n dimensions"""
    dim = len(bounds)
    
    # Generate random points
    points = np.random.uniform(
        low=[b[0] for b in bounds],
        high=[b[1] for b in bounds],
        size=(n, dim)
    )
    
    # Compute volume
    volume = np.prod([b[1] - b[0] for b in bounds])
    
    # Estimate integral
    return volume * np.mean(f(points))

# Example: âˆ«âˆ«e^(-(xÂ²+yÂ²))dxdy over [0,1]Ã—[0,1]
f = lambda p: np.exp(-np.sum(p**2, axis=1))
bounds = [(0, 1), (0, 1)]
result = mc_integrate_nd(f, bounds)
print(f"2D integral: {result:.6f}")
```

---

## ğŸ¯ AI/ML Applications

### 1. Probability Distributions

```python
from scipy.stats import norm

# PDF must integrate to 1
pdf = lambda x: (1/np.sqrt(2*np.pi)) * np.exp(-x**2/2)

# Verify normalization
total_prob, _ = integrate.quad(pdf, -np.inf, np.inf)
print(f"Total probability: {total_prob:.6f}")  # 1.0

# CDF via integration
def cdf_from_pdf(pdf, x):
    """Compute CDF from PDF"""
    result, _ = integrate.quad(pdf, -np.inf, x)
    return result

# Compare with scipy
x = 1.96
cdf_integrated = cdf_from_pdf(pdf, x)
cdf_scipy = norm.cdf(x)
print(f"Integrated CDF: {cdf_integrated:.6f}")
print(f"SciPy CDF: {cdf_scipy:.6f}")
```

### 2. Expected Value and Moments

```python
def expected_value(pdf, a, b):
    """E[X] = âˆ«xÂ·p(x)dx"""
    integrand = lambda x: x * pdf(x)
    return integrate.quad(integrand, a, b)[0]

def variance(pdf, a, b):
    """Var(X) = E[XÂ²] - (E[X])Â²"""
    mean = expected_value(pdf, a, b)
    integrand = lambda x: (x - mean)**2 * pdf(x)
    return integrate.quad(integrand, a, b)[0]

def nth_moment(pdf, n, a, b):
    """E[X^n] = âˆ«x^nÂ·p(x)dx"""
    integrand = lambda x: x**n * pdf(x)
    return integrate.quad(integrand, a, b)[0]

# Example: Uniform distribution on [0,1]
pdf = lambda x: 1.0
print(f"E[X] = {expected_value(pdf, 0, 1):.3f}")  # 0.5
print(f"Var(X) = {variance(pdf, 0, 1):.3f}")  # 1/12 â‰ˆ 0.083
```

### 3. Marginal Distributions

```python
# Marginalize joint distribution
def marginalize(joint_pdf, x_val):
    """p(x) = âˆ«p(x,y)dy"""
    integrand = lambda y: joint_pdf(x_val, y)
    result, _ = integrate.quad(integrand, -np.inf, np.inf)
    return result

# Example: Bivariate normal
from scipy.stats import multivariate_normal

mean = [0, 0]
cov = [[1, 0.5], [0.5, 1]]
mvn = multivariate_normal(mean, cov)

joint_pdf = lambda x, y: mvn.pdf([x, y])
marginal_x = marginalize(joint_pdf, 0)
print(f"Marginal p(x=0): {marginal_x:.6f}")
```

### 4. Loss Function Integration

```python
# Expected risk
def expected_risk(loss_fn, model, data_dist, x_range):
    """E[L(f(x), y)] over data distribution"""
    def integrand(x):
        # Assuming y = true_function(x) + noise
        y_true = true_function(x)
        y_pred = model(x)
        return loss_fn(y_pred, y_true) * data_dist(x)
    
    return integrate.quad(integrand, x_range[0], x_range[1])[0]
```

### 5. Variational Inference (ELBO)

```python
# Evidence Lower Bound
def elbo(q_dist, log_p, log_q, n_samples=10000):
    """ELBO = E_q[log p(x,z)] - E_q[log q(z)]"""
    samples = q_dist(n_samples)
    return np.mean(log_p(samples) - log_q(samples))
```

---

## ğŸ›¡ï¸ Numerical Stability

### Common Issues

**1. Infinite Limits**
```python
# BAD: May not converge
result = integrate.quad(f, 0, np.inf)

# GOOD: Use appropriate method
result = integrate.quad(f, 0, np.inf, limit=100)
# Or transform to finite interval
```

**2. Singularities**
```python
# Function with singularity at x=0
f = lambda x: 1/np.sqrt(x)

# SciPy handles this
result, error = integrate.quad(f, 0, 1)
print(f"Result: {result:.6f}")  # 2.0
```

**3. Oscillatory Functions**
```python
# Highly oscillatory integrand
f = lambda x: np.sin(100*x)

# Need more subdivisions
result = integrate.quad(f, 0, 1, limit=1000)
```

---

## ğŸ“ Advanced Exercises

### Exercise 1: Gaussian Integral
**Problem:** Prove that âˆ«â‚‹âˆ^âˆ e^(-xÂ²)dx = âˆšÏ€

**Hint:** Use polar coordinates and double integral

### Exercise 2: Implement Adaptive Quadrature
**Problem:** Write adaptive Simpson's rule that subdivides intervals where error is large

```python
def adaptive_simpson(f, a, b, tol=1e-6):
    """Adaptive Simpson's rule"""
    # Your implementation here
    pass
```

### Exercise 3: Monte Carlo Convergence
**Problem:** Show empirically that MC error decreases as O(1/âˆšn)

---

## ğŸ“ Interview Focus

### Key Questions

1.  **What is the fundamental theorem of calculus?**
    - Links differentiation and integration
    - âˆ«â‚áµ‡ f'(x)dx = f(b) - f(a)
    - Foundation of calculus

2.  **Why use numerical integration?**
    - No closed-form solution
    - Complex functions
    - Empirical data

3.  **Monte Carlo vs deterministic methods?**
    - MC: O(1/âˆšn), dimension-independent
    - Deterministic: O(1/n^(1/d)), curse of dimensionality
    - MC better for high dimensions

4.  **Integration in ML?**
    - Computing expectations
    - Marginalizing distributions
    - ELBO in variational inference

5.  **Importance sampling advantage?**
    - Reduces variance
    - Focuses samples where integrand is large
    - Critical for rare events

---

## ğŸ“š References

-   **Books:** 
    - "Calculus" - James Stewart
    - "Numerical Recipes" - Press et al.
    - "Monte Carlo Statistical Methods" - Robert & Casella

-   **Online:**
    - [SciPy Integration](https://docs.scipy.org/doc/scipy/tutorial/integrate.html)
    - [3Blue1Brown: Integration](https://www.youtube.com/watch?v=rfG8ce4nNh0)

---

**Integration: from areas to expectations, the foundation of probabilistic ML!**
