# Derivatives and Gradients

> **Foundation of optimization in AI** - Understanding how functions change

---

## ðŸ“ Derivatives

### Definition

The **derivative** measures the rate of change of a function.

**Formal Definition:**
```
f'(x) = lim[hâ†’0] (f(x+h) - f(x)) / h
```

**Notation:**
- f'(x) - Lagrange notation
- df/dx - Leibniz notation  
- Df(x) - Operator notation
- á¸Ÿ(x) - Newton notation

### Geometric Interpretation

- **Slope** of tangent line at point x
- **Instantaneous rate of change**
- Direction of steepest ascent

---

## ðŸ”¢ Basic Derivative Rules

### Power Rule
```
d/dx(xâ¿) = nxâ¿â»Â¹
```

### Constant Rule
```
d/dx(c) = 0
```

### Constant Multiple Rule
```
d/dx(cf(x)) = cÂ·f'(x)
```

### Sum/Difference Rule
```
d/dx(f(x) Â± g(x)) = f'(x) Â± g'(x)
```

### Product Rule
```
d/dx(f(x)g(x)) = f'(x)g(x) + f(x)g'(x)
```

### Quotient Rule
```
d/dx(f(x)/g(x)) = (f'(x)g(x) - f(x)g'(x)) / g(x)Â²
```

### Chain Rule
```
d/dx(f(g(x))) = f'(g(x))Â·g'(x)
```

---

## ðŸ“Š Common Derivatives

### Exponential & Logarithmic
```
d/dx(eË£) = eË£
d/dx(aË£) = aË£ ln(a)
d/dx(ln(x)) = 1/x
d/dx(logâ‚(x)) = 1/(x ln(a))
```

### Trigonometric
```
d/dx(sin(x)) = cos(x)
d/dx(cos(x)) = -sin(x)
d/dx(tan(x)) = secÂ²(x)
```

### Activation Functions (AI/ML)
```
d/dx(sigmoid(x)) = sigmoid(x)(1 - sigmoid(x))
d/dx(tanh(x)) = 1 - tanhÂ²(x)
d/dx(ReLU(x)) = 1 if x>0, else 0
d/dx(LeakyReLU(x)) = 1 if x>0, else Î±
```

---

## ðŸŽ¯ Partial Derivatives

For multivariable functions f(xâ‚, xâ‚‚, ..., xâ‚™):

**Partial derivative** with respect to xáµ¢:
```
âˆ‚f/âˆ‚xáµ¢ = lim[hâ†’0] (f(xâ‚,...,xáµ¢+h,...,xâ‚™) - f(xâ‚,...,xáµ¢,...,xâ‚™)) / h
```

**Notation:** âˆ‚f/âˆ‚xáµ¢ or fâ‚“áµ¢

**Example:**
```
f(x,y) = xÂ²y + 3xyÂ²

âˆ‚f/âˆ‚x = 2xy + 3yÂ²
âˆ‚f/âˆ‚y = xÂ² + 6xy
```

---

## ðŸ”º Gradient

The **gradient** is a vector of all partial derivatives.

**Definition:**
```
âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€
```

**Properties:**
- Points in direction of steepest ascent
- Perpendicular to level curves/surfaces
- Magnitude = rate of steepest increase

**Example:**
```
f(x,y) = xÂ² + yÂ²

âˆ‡f = [2x, 2y]áµ€

At point (1,1): âˆ‡f = [2, 2]áµ€
```

---

## ðŸŽ¨ Directional Derivative

Rate of change in direction **u** (unit vector):

```
Dáµ¤f(x) = âˆ‡f(x) Â· u
```

**Maximum rate of change** occurs when u is parallel to âˆ‡f:
```
max Dáµ¤f = ||âˆ‡f||
```

---

## ðŸ”§ Higher-Order Derivatives

### Second Derivative
```
f''(x) = dÂ²f/dxÂ²
```

**Interpretation:**
- Measures curvature
- f'' > 0: convex (curving up)
- f'' < 0: concave (curving down)

### Hessian Matrix

For f: â„â¿ â†’ â„, the **Hessian** is the matrix of second partial derivatives:

```
H = [âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼]

     [âˆ‚Â²f/âˆ‚xâ‚Â²    âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚  ...  âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚™]
H =  [âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚‚Â²    ...  âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚™]
     [    â‹®            â‹®       â‹±        â‹®     ]
     [âˆ‚Â²f/âˆ‚xâ‚™âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚™âˆ‚xâ‚‚  ...  âˆ‚Â²f/âˆ‚xâ‚™Â²  ]
```

**Properties:**
- Symmetric (if f is CÂ²)
- Describes local curvature
- Used in optimization (Newton's method)

---

## ðŸ’» Practical Workflows

### NumPy Implementation

```python
import numpy as np

# Numerical derivative (finite differences)
def numerical_derivative(f, x, h=1e-5):
    """Compute derivative using finite differences"""
    return (f(x + h) - f(x - h)) / (2 * h)

# Example
f = lambda x: x**2
x = 2.0
df_dx = numerical_derivative(f, x)
print(f"f'({x}) â‰ˆ {df_dx}")  # Should be â‰ˆ 4

# Gradient (multivariable)
def numerical_gradient(f, x, h=1e-5):
    """Compute gradient using finite differences"""
    grad = np.zeros_like(x)
    for i in range(len(x)):
        x_plus = x.copy()
        x_minus = x.copy()
        x_plus[i] += h
        x_minus[i] -= h
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)
    return grad

# Example
f = lambda x: x[0]**2 + x[1]**2
x = np.array([1.0, 1.0])
grad = numerical_gradient(f, x)
print(f"âˆ‡f({x}) â‰ˆ {grad}")  # Should be â‰ˆ [2, 2]

# Hessian
def numerical_hessian(f, x, h=1e-5):
    """Compute Hessian using finite differences"""
    n = len(x)
    H = np.zeros((n, n))
    
    for i in range(n):
        for j in range(n):
            x_pp = x.copy()
            x_pm = x.copy()
            x_mp = x.copy()
            x_mm = x.copy()
            
            x_pp[i] += h; x_pp[j] += h
            x_pm[i] += h; x_pm[j] -= h
            x_mp[i] -= h; x_mp[j] += h
            x_mm[i] -= h; x_mm[j] -= h
            
            H[i,j] = (f(x_pp) - f(x_pm) - f(x_mp) + f(x_mm)) / (4 * h**2)
    
    return H

# Example
H = numerical_hessian(f, x)
print(f"Hessian:\n{H}")  # Should be â‰ˆ [[2, 0], [0, 2]]
```

### Automatic Differentiation

```python
# Using JAX (recommended for ML)
import jax
import jax.numpy as jnp

# Define function
def f(x):
    return jnp.sum(x**2)

# Gradient
grad_f = jax.grad(f)
x = jnp.array([1.0, 2.0, 3.0])
print(f"Gradient: {grad_f(x)}")  # [2., 4., 6.]

# Hessian
hessian_f = jax.hessian(f)
print(f"Hessian:\n{hessian_f(x)}")

# Using PyTorch
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = torch.sum(x**2)

# Compute gradient
y.backward()
print(f"Gradient: {x.grad}")  # tensor([2., 4., 6.])
```

---

## ðŸŽ¯ AI/ML Applications

### 1. Gradient Descent

```python
def gradient_descent(f, grad_f, x0, lr=0.01, max_iter=1000):
    """Minimize f using gradient descent"""
    x = x0.copy()
    history = [x.copy()]
    
    for i in range(max_iter):
        grad = grad_f(x)
        x = x - lr * grad
        history.append(x.copy())
        
        if np.linalg.norm(grad) < 1e-6:
            break
    
    return x, history

# Example: minimize f(x,y) = xÂ² + yÂ²
f = lambda x: np.sum(x**2)
grad_f = lambda x: 2*x

x0 = np.array([5.0, 5.0])
x_min, history = gradient_descent(f, grad_f, x0, lr=0.1)
print(f"Minimum at: {x_min}")  # Should be â‰ˆ [0, 0]
```

### 2. Backpropagation

```python
# Simple neural network layer
class LinearLayer:
    def __init__(self, input_dim, output_dim):
        self.W = np.random.randn(input_dim, output_dim) * 0.01
        self.b = np.zeros(output_dim)
    
    def forward(self, x):
        self.x = x  # Cache for backward pass
        return x @ self.W + self.b
    
    def backward(self, grad_output):
        # Gradients
        self.grad_W = self.x.T @ grad_output
        self.grad_b = np.sum(grad_output, axis=0)
        grad_input = grad_output @ self.W.T
        return grad_input
    
    def update(self, lr=0.01):
        self.W -= lr * self.grad_W
        self.b -= lr * self.grad_b
```

### 3. Loss Function Gradients

```python
# MSE loss gradient
def mse_gradient(y_pred, y_true):
    """Gradient of MSE loss"""
    return 2 * (y_pred - y_true) / len(y_true)

# Cross-entropy loss gradient (with softmax)
def cross_entropy_gradient(logits, y_true):
    """Gradient of cross-entropy with softmax"""
    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    probs[range(len(y_true)), y_true] -= 1
    return probs / len(y_true)
```

---

## ðŸŽ“ Interview Focus

### Key Questions

1. **What is a derivative?**
   - Rate of change of function
   - Slope of tangent line
   - Limit of difference quotient

2. **What is the gradient?**
   - Vector of partial derivatives
   - Points in direction of steepest ascent
   - Used in optimization

3. **Chain rule in neural networks?**
   - Backpropagation uses chain rule
   - Compute gradients layer by layer
   - âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚w

4. **Why is gradient descent called "gradient" descent?**
   - Uses gradient to find direction
   - Moves opposite to gradient (descent)
   - Minimizes loss function

5. **What does Hessian tell us?**
   - Second-order curvature information
   - Positive definite â†’ local minimum
   - Used in Newton's method

### Must-Know Formulas

```
Chain rule: d/dx(f(g(x))) = f'(g(x))Â·g'(x)
Gradient: âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€
Gradient descent: x_{t+1} = x_t - Î·âˆ‡f(x_t)
Sigmoid derivative: Ïƒ'(x) = Ïƒ(x)(1-Ïƒ(x))
```

### Common Pitfalls

- âŒ Forgetting chain rule in backprop
- âŒ Not checking gradient numerically
- âŒ Confusing gradient with directional derivative
- âŒ Wrong sign in gradient descent (should be minus)

---

## ðŸ”— Connections

### Prerequisites
- Basic algebra
- Functions

### Related Topics
- [Chain Rule](Chain-Rule-and-Backpropagation.md)
- [Multivariate Calculus](Multivariate-Calculus.md)
- [Optimization](../4_Optimization/)

### Applications in AI
- **Gradient Descent:** Optimization algorithm
- **Backpropagation:** Training neural networks
- **Feature Importance:** Gradient magnitude
- **Adversarial Examples:** Gradient-based attacks

---

## ðŸ“š References

- **Books:**
  - "Calculus" - James Stewart
  - "Deep Learning" - Goodfellow et al. (Chapter 4)

- **Online:**
  - [3Blue1Brown: Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)
  - [Khan Academy: Calculus](https://www.khanacademy.org/math/calculus-1)

---

**Master derivatives and gradients - they power all of deep learning!**
