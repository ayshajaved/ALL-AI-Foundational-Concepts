# Tensor Calculus and Einstein Notation

> **Calculus on tensors** - Derivatives of tensor operations for deep learning

---

## ğŸ¯ Einstein Summation Convention (Review)

### Notation
```
Repeated indices are summed:
yáµ¢ = Î£â±¼ Aáµ¢â±¼xâ±¼  â†’  yáµ¢ = Aáµ¢â±¼xâ±¼

Free indices appear on both sides
Dummy indices (summed) appear only on right
```

---

## ğŸ“Š Tensor Derivatives

### Scalar-by-Vector
```
âˆ‚f/âˆ‚x = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€

Example: f(x) = xáµ€Ax
âˆ‚f/âˆ‚x = (A + Aáµ€)x
```

### Vector-by-Scalar
```
âˆ‚y/âˆ‚x where y âˆˆ â„áµ, x âˆˆ â„
Result: m Ã— 1 vector
```

### Vector-by-Vector (Jacobian)
```
âˆ‚y/âˆ‚x where y âˆˆ â„áµ, x âˆˆ â„â¿

J = [âˆ‚yáµ¢/âˆ‚xâ±¼]  (m Ã— n matrix)
```

```python
import torch

# Automatic differentiation
x = torch.randn(5, requires_grad=True)
y = x ** 2 + 2 * x

# Jacobian
J = torch.autograd.functional.jacobian(lambda x: x**2 + 2*x, x)
print(f"Jacobian shape: {J.shape}")  # (5, 5)
```

---

## ğŸ¯ Matrix Calculus

### Scalar-by-Matrix
```
âˆ‚f/âˆ‚A where f: â„áµË£â¿ â†’ â„

Result: m Ã— n matrix

Example: f(A) = tr(A)
âˆ‚f/âˆ‚A = I
```

### Common Derivatives

```python
# 1. Trace
# f(A) = tr(A) = Î£áµ¢ Aáµ¢áµ¢
# âˆ‚f/âˆ‚A = I

# 2. Frobenius norm
# f(A) = ||A||Â²_F = tr(Aáµ€A)
# âˆ‚f/âˆ‚A = 2A

# 3. Determinant
# f(A) = det(A)
# âˆ‚f/âˆ‚A = det(A)Â·Aâ»áµ€

# 4. Inverse
# f(A) = Aâ»Â¹
# âˆ‚vec(Aâ»Â¹)/âˆ‚vec(A) = -(Aâ»áµ€ âŠ— Aâ»Â¹)
```

---

## ğŸ“ˆ Tensor-by-Tensor Derivatives

### General Form
```
âˆ‚Y/âˆ‚X where Y, X are tensors

Result: higher-order tensor
```

### Example: Matrix-by-Matrix

```python
def matrix_by_matrix_derivative():
    """
    Example: Y = AXB
    âˆ‚Y/âˆ‚X in tensor form
    """
    # Using einsum notation
    # âˆ‚Yáµ¢â±¼/âˆ‚Xâ‚–â‚— = Aáµ¢â‚–Bâ‚—â±¼
    
    A = torch.randn(3, 4)
    X = torch.randn(4, 5, requires_grad=True)
    B = torch.randn(5, 6)
    
    Y = A @ X @ B
    
    # Compute full derivative tensor
    # Shape: (3, 6, 4, 5)
    dY_dX = torch.zeros(3, 6, 4, 5)
    
    for i in range(3):
        for j in range(6):
            for k in range(4):
                for l in range(5):
                    dY_dX[i, j, k, l] = A[i, k] * B[l, j]
    
    return dY_dX
```

---

## ğŸ¯ Chain Rule for Tensors

### Scalar Chain Rule
```
f(g(x)):
âˆ‚f/âˆ‚x = (âˆ‚f/âˆ‚g)(âˆ‚g/âˆ‚x)
```

### Tensor Chain Rule
```
Y = f(g(X)):
âˆ‚Y/âˆ‚X = Î£ (âˆ‚Y/âˆ‚g)(âˆ‚g/âˆ‚X)

Sum over intermediate indices
```

### Example: Backpropagation

```python
# Forward: y = Ïƒ(Wx + b)
# Backward: âˆ‚L/âˆ‚W

def backprop_example():
    """
    L = loss(y)
    y = Ïƒ(z)
    z = Wx + b
    
    âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚W
           = âˆ‚L/âˆ‚y Â· Ïƒ'(z) Â· xáµ€
    """
    # Dimensions
    batch, in_dim, out_dim = 32, 10, 5
    
    x = torch.randn(batch, in_dim)
    W = torch.randn(out_dim, in_dim, requires_grad=True)
    b = torch.randn(out_dim, requires_grad=True)
    
    # Forward
    z = x @ W.T + b  # (batch, out_dim)
    y = torch.sigmoid(z)
    
    # Loss (example)
    L = y.sum()
    
    # Backward
    L.backward()
    
    print(f"âˆ‚L/âˆ‚W shape: {W.grad.shape}")  # (out_dim, in_dim)
    
    return W.grad
```

---

## ğŸ“Š Useful Identities

### Matrix Derivatives

```python
# 1. âˆ‚(Ax)/âˆ‚x = Aáµ€
# 2. âˆ‚(xáµ€A)/âˆ‚x = A
# 3. âˆ‚(xáµ€Ax)/âˆ‚x = (A + Aáµ€)x
# 4. âˆ‚(xáµ€Ay)/âˆ‚x = Ay
# 5. âˆ‚tr(AB)/âˆ‚A = Báµ€
# 6. âˆ‚tr(ABA^TC)/âˆ‚A = CAB + Cáµ€ABáµ€
# 7. âˆ‚log det(A)/âˆ‚A = Aâ»áµ€
```

---

## ğŸ¯ Applications in Deep Learning

### 1. Attention Mechanism

```python
# Attention: softmax(QKáµ€/âˆšd)V
# Derivatives needed for backprop

def attention_derivative():
    """
    A = softmax(QKáµ€/âˆšd)
    Y = AV
    
    âˆ‚L/âˆ‚Q, âˆ‚L/âˆ‚K, âˆ‚L/âˆ‚V
    """
    d = 64
    Q = torch.randn(10, d, requires_grad=True)
    K = torch.randn(10, d, requires_grad=True)
    V = torch.randn(10, d, requires_grad=True)
    
    # Forward
    scores = Q @ K.T / np.sqrt(d)
    A = torch.softmax(scores, dim=-1)
    Y = A @ V
    
    # Loss
    L = Y.sum()
    L.backward()
    
    return Q.grad, K.grad, V.grad
```

### 2. Batch Normalization

```python
# y = Î³(x - Î¼)/Ïƒ + Î²
# âˆ‚L/âˆ‚x involves âˆ‚Î¼/âˆ‚x and âˆ‚Ïƒ/âˆ‚x

def batchnorm_derivative():
    """
    Batch normalization gradient
    """
    batch, dim = 32, 10
    x = torch.randn(batch, dim, requires_grad=True)
    gamma = torch.ones(dim, requires_grad=True)
    beta = torch.zeros(dim, requires_grad=True)
    
    # Forward
    mu = x.mean(dim=0)
    var = x.var(dim=0, unbiased=False)
    x_norm = (x - mu) / torch.sqrt(var + 1e-5)
    y = gamma * x_norm + beta
    
    # Loss
    L = y.sum()
    L.backward()
    
    return x.grad
```

---

## ğŸ“ Interview Focus

### Key Questions

1. **Einstein notation benefits?**
   - Concise tensor operations
   - Automatic summation
   - Used in einsum

2. **Jacobian vs Hessian?**
   - Jacobian: vector-by-vector (1st order)
   - Hessian: scalar-by-vector twice (2nd order)

3. **Chain rule in backprop?**
   - Multiply Jacobians
   - Efficient with reverse mode
   - O(n) for n parameters

4. **Why matrix calculus?**
   - Derive gradient formulas
   - Understand backprop
   - Optimize implementations

---

## ğŸ“š References

- **Books:**
  - "The Matrix Cookbook" - Petersen & Pedersen
  - "Matrix Differential Calculus with Applications" - Magnus & Neudecker

---

**Tensor calculus: the language of deep learning gradients!**
