# Convex Optimization

> **The foundation of tractable optimization** - When global optimum is guaranteed

---

## ğŸ¯ Convex Sets

### Definition
A set C is **convex** if for any x, y âˆˆ C and Î» âˆˆ [0,1]:
```
Î»x + (1-Î»)y âˆˆ C
```

**Intuition:** Line segment between any two points stays in set

### Examples
- **Convex:** Hyperplanes, halfspaces, balls, ellipsoids
- **Not convex:** Union of disjoint sets, non-convex polygons

---

## ğŸ“ˆ Convex Functions

### Definition
f is **convex** if for all x, y and Î» âˆˆ [0,1]:
```
f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y)
```

### First-Order Condition
f is convex iff:
```
f(y) â‰¥ f(x) + âˆ‡f(x)áµ€(y - x)
```

### Second-Order Condition
f is convex iff Hessian is positive semidefinite:
```
âˆ‡Â²f(x) âª° 0  for all x
```

### Examples
```python
# Convex functions
f(x) = xÂ²                    # Quadratic
f(x) = eË£                    # Exponential
f(x) = -log(x)              # Negative log
f(x) = ||x||                # Norm
f(x) = max(xâ‚, xâ‚‚, ..., xâ‚™) # Max

# Not convex
f(x) = xÂ³                    # Cubic
f(x) = sin(x)               # Trigonometric
```

---

## ğŸ¯ Convex Optimization Problem

### Standard Form
```
minimize    f(x)
subject to  gáµ¢(x) â‰¤ 0,  i = 1,...,m
            hâ±¼(x) = 0,  j = 1,...,p

where f, gáµ¢ are convex, hâ±¼ are affine
```

### Key Property
**Any local minimum is a global minimum!**

---

## ğŸ“Š Common Convex Problems

### 1. Linear Programming (LP)
```
minimize    cáµ€x
subject to  Ax â‰¤ b
```

### 2. Quadratic Programming (QP)
```
minimize    Â½xáµ€Qx + cáµ€x
subject to  Ax â‰¤ b

where Q âª° 0
```

### 3. Least Squares
```
minimize    ||Ax - b||Â²
```

**Closed-form solution:**
```
x* = (Aáµ€A)â»Â¹Aáµ€b
```

### 4. Ridge Regression
```
minimize    ||Ax - b||Â² + Î»||x||Â²
```

**Solution:**
```
x* = (Aáµ€A + Î»I)â»Â¹Aáµ€b
```

### 5. Lasso (L1 Regularization)
```
minimize    ||Ax - b||Â² + Î»||x||â‚
```

**No closed form, use iterative methods**

---

## ğŸ¯ Optimality Conditions

### Unconstrained
**Necessary:** âˆ‡f(x*) = 0
**Sufficient (convex):** âˆ‡f(x*) = 0

### Constrained (KKT Conditions)
For convex problem, x* is optimal iff:
```
1. Stationarity: âˆ‡f(x*) + Î£Î»áµ¢âˆ‡gáµ¢(x*) + Î£Î½â±¼âˆ‡hâ±¼(x*) = 0
2. Primal feasibility: gáµ¢(x*) â‰¤ 0, hâ±¼(x*) = 0
3. Dual feasibility: Î»áµ¢ â‰¥ 0
4. Complementary slackness: Î»áµ¢gáµ¢(x*) = 0
```

---

## ğŸ’» Practical Implementation

```python
import numpy as np
from scipy.optimize import minimize

# Convex quadratic function
def f(x):
    return 0.5 * x.T @ Q @ x + c.T @ x

def grad_f(x):
    return Q @ x + c

# Example: minimize Â½xáµ€Qx + cáµ€x
Q = np.array([[2, 0], [0, 2]])  # Positive definite
c = np.array([1, 1])

# Using scipy
result = minimize(f, x0=np.zeros(2), jac=grad_f, method='BFGS')
print(f"Optimal x: {result.x}")

# Analytical solution
x_opt = -np.linalg.solve(Q, c)
print(f"Analytical: {x_opt}")
```

---

## ğŸ“ Interview Focus

### Key Questions

1. **What is a convex function?**
   - f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y)
   - Any local min is global min

2. **Why convexity matters?**
   - Guaranteed global optimum
   - Efficient algorithms
   - Tractable analysis

3. **Is neural network training convex?**
   - No! Non-convex due to composition
   - Multiple local minima
   - No global optimum guarantee

4. **Convex relaxation?**
   - Approximate non-convex with convex
   - Get lower bound
   - Used in combinatorial optimization

---

## ğŸ“š References

- **Books:** "Convex Optimization" - Boyd & Vandenberghe

---

**Convex optimization: when we can find the best solution!**
