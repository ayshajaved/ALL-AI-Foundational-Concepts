# Constrained Optimization

> **Optimization with constraints** - Lagrange multipliers and KKT conditions

---

## ðŸŽ¯ Problem Formulation

### General Form
```
minimize    f(x)
subject to  gáµ¢(x) â‰¤ 0,  i = 1,...,m  (inequality)
            hâ±¼(x) = 0,  j = 1,...,p  (equality)
```

---

## ðŸ“Š Lagrange Multipliers

### Equality Constraints Only
```
minimize    f(x)
subject to  h(x) = 0
```

**Lagrangian:**
```
L(x, Î½) = f(x) + Î½h(x)
```

**Optimality:** âˆ‡â‚“L = 0 and h(x) = 0

### Example
```python
# Minimize xÂ² + yÂ² subject to x + y = 1
# L = xÂ² + yÂ² + Î½(x + y - 1)
# âˆ‚L/âˆ‚x = 2x + Î½ = 0
# âˆ‚L/âˆ‚y = 2y + Î½ = 0
# x + y = 1
# Solution: x = y = 0.5
```

---

## ðŸŽ¯ KKT Conditions

### For General Constrained Problem

**Necessary conditions (if constraint qualification holds):**

1. **Stationarity:**
   ```
   âˆ‡f(x*) + Î£Î»áµ¢âˆ‡gáµ¢(x*) + Î£Î½â±¼âˆ‡hâ±¼(x*) = 0
   ```

2. **Primal feasibility:**
   ```
   gáµ¢(x*) â‰¤ 0,  hâ±¼(x*) = 0
   ```

3. **Dual feasibility:**
   ```
   Î»áµ¢ â‰¥ 0
   ```

4. **Complementary slackness:**
   ```
   Î»áµ¢gáµ¢(x*) = 0
   ```

**For convex problems:** KKT conditions are sufficient!

---

## ðŸ’» Practical Methods

### 1. Penalty Method
```python
def penalty_method(f, g, x0, rho=1.0, max_iter=100):
    """Quadratic penalty method"""
    x = x0.copy()
    
    for k in range(max_iter):
        # Penalized objective
        def f_penalty(x):
            penalty = sum(max(0, gi(x))**2 for gi in g)
            return f(x) + rho * penalty
        
        # Minimize unconstrained problem
        x = minimize(f_penalty, x).x
        
        # Increase penalty
        rho *= 2
    
    return x
```

### 2. Augmented Lagrangian
```python
def augmented_lagrangian(f, g, h, x0, max_iter=100):
    """Augmented Lagrangian method"""
    x = x0.copy()
    lambda_g = np.zeros(len(g))
    nu_h = np.zeros(len(h))
    rho = 1.0
    
    for k in range(max_iter):
        # Augmented Lagrangian
        def L_aug(x):
            L = f(x)
            # Inequality constraints
            for i, gi in enumerate(g):
                L += lambda_g[i] * gi(x) + rho/2 * max(0, gi(x))**2
            # Equality constraints
            for j, hj in enumerate(h):
                L += nu_h[j] * hj(x) + rho/2 * hj(x)**2
            return L
        
        # Minimize
        x = minimize(L_aug, x).x
        
        # Update multipliers
        for i, gi in enumerate(g):
            lambda_g[i] = max(0, lambda_g[i] + rho * gi(x))
        for j, hj in enumerate(h):
            nu_h[j] += rho * hj(x)
    
    return x
```

---

## ðŸ“š References

- **Books:** "Numerical Optimization" - Nocedal & Wright

---

**Constrained optimization: real-world problems have constraints!**
