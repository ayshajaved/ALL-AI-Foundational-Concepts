# Matrix Operations

> **Essential matrix manipulations for AI** - Determinants, inverses, rank, and trace

---

## ğŸ”¢ Determinant

### Definition
The **determinant** is a scalar value that encodes properties of a square matrix.

**Notation:** det(A) or |A|

### 2Ã—2 Matrix
```
A = [a  b]
    [c  d]

det(A) = ad - bc
```

### 3Ã—3 Matrix (Cofactor Expansion)
```
A = [a  b  c]
    [d  e  f]
    [g  h  i]

det(A) = a(ei-fh) - b(di-fg) + c(dh-eg)
```

### Properties

1. **det(I) = 1**
2. **det(AB) = det(A)det(B)**
3. **det(Aáµ€) = det(A)**
4. **det(Aâ»Â¹) = 1/det(A)**
5. **det(Î±A) = Î±â¿det(A)** (n = matrix size)
6. **Row swap changes sign**
7. **det(A) = 0 âŸº A is singular (not invertible)**

### Geometric Interpretation
- **2D:** Area of parallelogram formed by column vectors
- **3D:** Volume of parallelepiped
- **Sign:** Orientation (positive = right-handed)

---

## ğŸ”„ Matrix Inverse

### Definition
For square matrix A, the **inverse** Aâ»Â¹ satisfies:
```
AAâ»Â¹ = Aâ»Â¹A = I
```

### Existence
Aâ»Â¹ exists if and only if:
- A is square
- det(A) â‰  0 (A is non-singular)

### 2Ã—2 Inverse
```
A = [a  b]      Aâ»Â¹ = 1/(ad-bc) [ d  -b]
    [c  d]                       [-c   a]
```

### Properties

1. **(Aâ»Â¹)â»Â¹ = A**
2. **(AB)â»Â¹ = Bâ»Â¹Aâ»Â¹** (reverse order!)
3. **(Aáµ€)â»Â¹ = (Aâ»Â¹)áµ€**
4. **det(Aâ»Â¹) = 1/det(A)**

### Computing Inverse

**Method 1: Gauss-Jordan Elimination**
```
[A | I] â†’ [I | Aâ»Â¹]
```

**Method 2: Adjugate Matrix**
```
Aâ»Â¹ = (1/det(A)) Ã— adj(A)
```

**Method 3: LU Decomposition** (efficient for large matrices)

---

## ğŸ“Š Rank

### Definition
The **rank** of a matrix is the dimension of the vector space spanned by its columns (or rows).

**Notation:** rank(A)

### Properties

1. **rank(A) â‰¤ min(m, n)** for mÃ—n matrix
2. **rank(A) = rank(Aáµ€)**
3. **rank(AB) â‰¤ min(rank(A), rank(B))**
4. **Full rank:** rank(A) = min(m, n)

### Types

**Full Column Rank:**
- rank(A) = n (number of columns)
- Columns are linearly independent
- Aáµ€ A is invertible

**Full Row Rank:**
- rank(A) = m (number of rows)
- Rows are linearly independent
- AAáµ€ is invertible

**Rank Deficient:**
- rank(A) < min(m, n)
- Columns/rows are linearly dependent

### Computing Rank
- Count non-zero rows in row echelon form
- Count non-zero singular values
- Use SVD

---

## ğŸ¯ Trace

### Definition
The **trace** is the sum of diagonal elements of a square matrix.

```
tr(A) = Î£áµ¢ aáµ¢áµ¢ = aâ‚â‚ + aâ‚‚â‚‚ + ... + aâ‚™â‚™
```

### Properties

1. **tr(A + B) = tr(A) + tr(B)**
2. **tr(Î±A) = Î±Â·tr(A)**
3. **tr(AB) = tr(BA)** (cyclic property)
4. **tr(ABC) = tr(CAB) = tr(BCA)**
5. **tr(Aáµ€) = tr(A)**
6. **tr(A) = sum of eigenvalues**

### Applications in AI

**1. Frobenius Norm**
```
||A||_F = âˆštr(Aáµ€A)
```

**2. Regularization**
```
L = Loss + Î»Â·tr(Wáµ€W)
```

**3. Kernel Methods**
```
K(x,y) = tr(Ï†(x)Ï†(y)áµ€)
```

---

## ğŸ”§ Matrix Decompositions (Overview)

### LU Decomposition
```
A = LU
L: Lower triangular
U: Upper triangular

Used for: Solving linear systems efficiently
```

### QR Decomposition
```
A = QR
Q: Orthogonal matrix
R: Upper triangular

Used for: Least squares, eigenvalue algorithms
```

### Cholesky Decomposition
```
A = LLáµ€  (for positive definite A)
L: Lower triangular

Used for: Efficient solving, numerical stability
```

---

## ğŸ’» Practical Workflows

### NumPy Implementation

```python
import numpy as np

# Create matrix
A = np.array([[1, 2], [3, 4]])

# Determinant
det_A = np.linalg.det(A)
print(f"Determinant: {det_A}")  # -2.0

# Inverse
A_inv = np.linalg.inv(A)
print(f"Inverse:\n{A_inv}")

# Verify: AAâ»Â¹ = I
identity = A @ A_inv
print(f"AAâ»Â¹:\n{identity}")

# Rank
rank_A = np.linalg.matrix_rank(A)
print(f"Rank: {rank_A}")  # 2 (full rank)

# Trace
trace_A = np.trace(A)
print(f"Trace: {trace_A}")  # 5

# Check if invertible
is_invertible = det_A != 0
print(f"Invertible: {is_invertible}")

# Solve linear system Ax = b
b = np.array([5, 11])
x = np.linalg.solve(A, b)
print(f"Solution: {x}")  # [1, 2]

# LU decomposition
from scipy.linalg import lu
P, L, U = lu(A)
print(f"L:\n{L}")
print(f"U:\n{U}")

# QR decomposition
Q, R = np.linalg.qr(A)
print(f"Q:\n{Q}")
print(f"R:\n{R}")
```

### Common Patterns

**1. Solving Linear Systems**
```python
# Instead of computing inverse
# DON'T: x = np.linalg.inv(A) @ b  # Slow, numerically unstable
# DO: x = np.linalg.solve(A, b)    # Fast, stable
```

**2. Checking Invertibility**
```python
def is_invertible(A):
    return np.linalg.det(A) != 0  # or check rank

# Better: use condition number
cond = np.linalg.cond(A)
if cond < 1e10:  # Well-conditioned
    A_inv = np.linalg.inv(A)
```

**3. Pseudo-Inverse (for non-square matrices)**
```python
# Moore-Penrose pseudo-inverse
A_pinv = np.linalg.pinv(A)
# Useful when A is not invertible
```

---

## ğŸ¯ AI/ML Applications

### 1. Solving Normal Equations
```python
# Linear regression: Î² = (Xáµ€X)â»Â¹Xáµ€y
X = np.random.randn(100, 5)
y = np.random.randn(100)

# Method 1: Using inverse (not recommended)
beta = np.linalg.inv(X.T @ X) @ X.T @ y

# Method 2: Using solve (better)
beta = np.linalg.solve(X.T @ X, X.T @ y)

# Method 3: Using lstsq (best)
beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)
```

### 2. Covariance Matrix
```python
# Covariance is symmetric, positive semi-definite
X = np.random.randn(100, 10)
X_centered = X - X.mean(axis=0)
Cov = (X_centered.T @ X_centered) / (len(X) - 1)

# Properties
assert np.allclose(Cov, Cov.T)  # Symmetric
eigenvalues = np.linalg.eigvals(Cov)
assert np.all(eigenvalues >= -1e-10)  # Non-negative eigenvalues
```

### 3. Regularization
```python
# Ridge regression: Î² = (Xáµ€X + Î»I)â»Â¹Xáµ€y
lambda_reg = 0.1
I = np.eye(X.shape[1])
beta_ridge = np.linalg.solve(X.T @ X + lambda_reg * I, X.T @ y)
```

---

## ğŸ“ Interview Focus

### Key Questions

1. **When is a matrix invertible?**
   - Square matrix
   - det(A) â‰  0
   - Full rank
   - All eigenvalues non-zero

2. **What does determinant = 0 mean?**
   - Matrix is singular (not invertible)
   - Columns are linearly dependent
   - Transforms space to lower dimension

3. **Why not compute inverse directly?**
   - Numerically unstable
   - Expensive (O(nÂ³))
   - Use `solve()` instead

4. **What is the difference between rank and dimension?**
   - Rank: dimension of column/row space
   - Dimension: size of matrix (mÃ—n)

5. **What does trace represent?**
   - Sum of diagonal elements
   - Sum of eigenvalues
   - Invariant under similarity transformations

### Must-Know Formulas

```
det(AB) = det(A)det(B)
(AB)â»Â¹ = Bâ»Â¹Aâ»Â¹
rank(AB) â‰¤ min(rank(A), rank(B))
tr(AB) = tr(BA)
```

### Common Pitfalls

- âŒ Computing inverse when not needed
- âŒ Not checking if matrix is invertible
- âŒ Forgetting (AB)â»Â¹ = Bâ»Â¹Aâ»Â¹ (reverse order)
- âŒ Assuming det(A+B) = det(A) + det(B) (FALSE!)

---

## ğŸ”— Connections

### Prerequisites
- [Vectors and Matrices](Vectors-and-Matrices.md)

### Related Topics
- [Eigenvalues and Eigenvectors](Eigenvalues-and-Eigenvectors.md)
- [SVD](SVD-and-Matrix-Decomposition.md)
- Linear systems solving

### Applications in AI
- Linear regression (normal equations)
- Covariance matrices
- Neural network weight updates
- Dimensionality reduction

---

## ğŸ“š References

- **Books:**
  - "Linear Algebra and Its Applications" - Gilbert Strang
  - "Matrix Computations" - Golub & Van Loan

- **Online:**
  - [NumPy Linear Algebra](https://numpy.org/doc/stable/reference/routines.linalg.html)
  - [SciPy Linear Algebra](https://docs.scipy.org/doc/scipy/reference/linalg.html)

---

**Master these operations - they power all of linear algebra!**
