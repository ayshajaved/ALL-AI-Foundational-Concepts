# Vectors and Matrices

> **Foundation of linear algebra for AI** - Understanding vectors and matrices is essential for all machine learning

---

## ðŸ“ Vectors

### Definition
A **vector** is an ordered array of numbers representing magnitude and direction in n-dimensional space.

**Notation:**
```
v = [vâ‚, vâ‚‚, ..., vâ‚™]áµ€  (column vector)
v = [vâ‚, vâ‚‚, ..., vâ‚™]   (row vector)
```

### Types of Vectors

**1. Zero Vector**
```
0 = [0, 0, ..., 0]áµ€
```

**2. Unit Vector** (length = 1)
```
||e|| = 1
Standard basis: eâ‚ = [1,0,0], eâ‚‚ = [0,1,0], eâ‚ƒ = [0,0,1]
```

**3. Sparse Vector** (mostly zeros)
- Common in NLP (word embeddings)
- Efficient storage

---

## ðŸ”¢ Vector Operations

### Addition
```
u + v = [uâ‚+vâ‚, uâ‚‚+vâ‚‚, ..., uâ‚™+vâ‚™]áµ€
```

**Properties:**
- Commutative: u + v = v + u
- Associative: (u + v) + w = u + (v + w)

### Scalar Multiplication
```
Î±v = [Î±vâ‚, Î±vâ‚‚, ..., Î±vâ‚™]áµ€
```

### Dot Product (Inner Product)
```
u Â· v = uâ‚vâ‚ + uâ‚‚vâ‚‚ + ... + uâ‚™vâ‚™ = Î£áµ¢ uáµ¢váµ¢
```

**Geometric Interpretation:**
```
u Â· v = ||u|| ||v|| cos(Î¸)
where Î¸ is angle between vectors
```

**Properties:**
- Commutative: u Â· v = v Â· u
- Distributive: u Â· (v + w) = u Â· v + u Â· w
- If u Â· v = 0, vectors are orthogonal

### Vector Norm (Length)

**L2 Norm (Euclidean)**
```
||v||â‚‚ = âˆš(vâ‚Â² + vâ‚‚Â² + ... + vâ‚™Â²) = âˆš(v Â· v)
```

**L1 Norm (Manhattan)**
```
||v||â‚ = |vâ‚| + |vâ‚‚| + ... + |vâ‚™|
```

**Lâˆž Norm (Maximum)**
```
||v||âˆž = max(|vâ‚|, |vâ‚‚|, ..., |vâ‚™|)
```

**p-Norm (General)**
```
||v||â‚š = (|vâ‚|áµ– + |vâ‚‚|áµ– + ... + |vâ‚™|áµ–)^(1/p)
```

### Unit Vector (Normalization)
```
vÌ‚ = v / ||v||
```

---

## ðŸ“Š Matrices

### Definition
A **matrix** is a rectangular array of numbers arranged in rows and columns.

**Notation:**
```
A = [aáµ¢â±¼]  where i = row, j = column

     [aâ‚â‚  aâ‚â‚‚  ...  aâ‚â‚™]
A =  [aâ‚‚â‚  aâ‚‚â‚‚  ...  aâ‚‚â‚™]
     [ â‹®    â‹®    â‹±    â‹® ]
     [aâ‚˜â‚  aâ‚˜â‚‚  ...  aâ‚˜â‚™]

Dimensions: m Ã— n (m rows, n columns)
```

### Special Matrices

**1. Square Matrix** (m = n)
```
A is n Ã— n
```

**2. Identity Matrix**
```
     [1  0  0]
I =  [0  1  0]
     [0  0  1]

AI = IA = A
```

**3. Diagonal Matrix**
```
     [dâ‚  0   0 ]
D =  [0   dâ‚‚  0 ]
     [0   0   dâ‚ƒ]

All off-diagonal elements = 0
```

**4. Zero Matrix**
```
All elements = 0
```

**5. Symmetric Matrix**
```
A = Aáµ€  (aáµ¢â±¼ = aâ±¼áµ¢)
```

**6. Orthogonal Matrix**
```
Qáµ€Q = QQáµ€ = I
Columns are orthonormal vectors
```

**7. Triangular Matrices**
```
Upper Triangular: aáµ¢â±¼ = 0 for i > j
Lower Triangular: aáµ¢â±¼ = 0 for i < j
```

---

## ðŸ”§ Matrix Operations

### Addition
```
C = A + B
cáµ¢â±¼ = aáµ¢â±¼ + báµ¢â±¼

Requires: Same dimensions
```

### Scalar Multiplication
```
B = Î±A
báµ¢â±¼ = Î±Â·aáµ¢â±¼
```

### Transpose
```
(Aáµ€)áµ¢â±¼ = Aâ±¼áµ¢

If A is mÃ—n, then Aáµ€ is nÃ—m
```

**Properties:**
- (Aáµ€)áµ€ = A
- (A + B)áµ€ = Aáµ€ + Báµ€
- (AB)áµ€ = Báµ€Aáµ€
- (Î±A)áµ€ = Î±Aáµ€

### Matrix-Vector Multiplication
```
y = Ax

yáµ¢ = Î£â±¼ aáµ¢â±¼xâ±¼

If A is mÃ—n and x is nÃ—1, then y is mÃ—1
```

**Interpretation:**
- Linear transformation
- Combination of column vectors

### Matrix-Matrix Multiplication
```
C = AB

cáµ¢â±¼ = Î£â‚– aáµ¢â‚–bâ‚–â±¼

If A is mÃ—n and B is nÃ—p, then C is mÃ—p
```

**Properties:**
- NOT commutative: AB â‰  BA (in general)
- Associative: (AB)C = A(BC)
- Distributive: A(B+C) = AB + AC

---

## ðŸŽ¯ AI/ML Applications

### 1. Data Representation
```python
# Dataset as matrix
X = [xâ‚, xâ‚‚, ..., xâ‚™]áµ€  # n samples
Each xáµ¢ is d-dimensional feature vector

X is n Ã— d matrix
```

### 2. Linear Transformations
```python
# Neural network layer
y = Wx + b

W: weight matrix
x: input vector
b: bias vector
y: output vector
```

### 3. Image Representation
```python
# Grayscale image: 2D matrix
# RGB image: 3D tensor (H Ã— W Ã— 3)
# Batch of images: 4D tensor (N Ã— H Ã— W Ã— 3)
```

### 4. Word Embeddings
```python
# Embedding matrix
E: V Ã— d  (V = vocabulary size, d = embedding dim)
Each row = word vector
```

---

## ðŸ’» Practical Workflows

### NumPy Implementation

```python
import numpy as np

# Create vectors
v = np.array([1, 2, 3])
u = np.array([4, 5, 6])

# Vector operations
v_plus_u = v + u
v_scaled = 2 * v
dot_product = np.dot(v, u)  # or v @ u
norm = np.linalg.norm(v)  # L2 norm
unit_vector = v / norm

# Create matrices
A = np.array([[1, 2], [3, 4], [5, 6]])  # 3Ã—2
B = np.array([[7, 8, 9], [10, 11, 12]])  # 2Ã—3

# Matrix operations
A_transpose = A.T
C = A @ B  # Matrix multiplication (3Ã—3)
identity = np.eye(3)  # 3Ã—3 identity

# Special matrices
zeros = np.zeros((3, 4))
ones = np.ones((2, 3))
diagonal = np.diag([1, 2, 3])

# Matrix-vector multiplication
x = np.array([1, 2])
y = A @ x  # Result: 3Ã—1

# Element-wise operations
A_squared = A ** 2  # Element-wise square
A_times_2 = A * 2   # Element-wise multiplication
```

### Common Patterns

**1. Batch Processing**
```python
# Process multiple samples at once
X = np.random.randn(100, 784)  # 100 samples, 784 features
W = np.random.randn(784, 10)   # Weights
Y = X @ W  # (100, 10) - all samples processed together
```

**2. Broadcasting**
```python
# Add bias to all samples
X = np.random.randn(100, 10)
b = np.random.randn(10)
Y = X + b  # b is broadcast to (100, 10)
```

**3. Normalization**
```python
# Normalize each feature
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_normalized = (X - X_mean) / X_std
```

---

## ðŸŽ“ Interview Focus

### Key Questions

1. **What is the difference between a row vector and column vector?**
   - Row: 1Ã—n, Column: nÃ—1
   - Transpose relationship
   - Different multiplication rules

2. **When can you multiply two matrices?**
   - A(mÃ—n) Ã— B(nÃ—p) = C(mÃ—p)
   - Inner dimensions must match

3. **What does the dot product represent geometrically?**
   - Projection of one vector onto another
   - Measures similarity/alignment
   - Zero if orthogonal

4. **Why is matrix multiplication not commutative?**
   - AB â‰  BA in general
   - Dimensions may not even match
   - Represents different transformations

5. **What is a symmetric matrix and why is it important?**
   - A = Aáµ€
   - Real eigenvalues
   - Common in covariance matrices

### Must-Know Formulas

```
Dot product: u Â· v = Î£áµ¢ uáµ¢váµ¢
L2 norm: ||v|| = âˆš(Î£áµ¢ váµ¢Â²)
Matrix mult: (AB)áµ¢â±¼ = Î£â‚– aáµ¢â‚–bâ‚–â±¼
Transpose: (AB)áµ€ = Báµ€Aáµ€
```

### Common Pitfalls

- âŒ Forgetting dimension compatibility
- âŒ Confusing element-wise and matrix multiplication
- âŒ Not checking for square matrices when needed
- âŒ Assuming commutativity

---

## ðŸ”— Connections

### Prerequisites
- Basic algebra
- Coordinate systems

### Related Topics
- [Matrix Operations](Matrix-Operations.md)
- [Eigenvalues and Eigenvectors](Eigenvalues-and-Eigenvectors.md)
- [Linear Transformations](../2_Calculus/Derivatives-and-Gradients.md)

### Applications in AI
- Neural network layers
- Data preprocessing
- Dimensionality reduction (PCA)
- Embeddings

---

## ðŸ“š References

- **Books:**
  - "Introduction to Linear Algebra" - Gilbert Strang
  - "Linear Algebra and Its Applications" - David Lay
  
- **Online:**
  - [3Blue1Brown: Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
  - [MIT OCW: Linear Algebra](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/)
  
- **Practice:**
  - NumPy documentation
  - Linear algebra exercises on Khan Academy

---

**Master vectors and matrices - they are the language of AI!**
