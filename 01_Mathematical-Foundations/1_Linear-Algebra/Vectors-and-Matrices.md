# Vectors and Matrices

> **Foundation of linear algebra for AI** - Understanding vectors and matrices is essential for all machine learning

---

## üìê Vectors

### Definition
A **vector** is an ordered array of numbers representing magnitude and direction in n-dimensional space.

**Notation:**
```
v = [v‚ÇÅ, v‚ÇÇ, ..., v‚Çô]·µÄ  (column vector)
v = [v‚ÇÅ, v‚ÇÇ, ..., v‚Çô]   (row vector)
```

### Types of Vectors

**1. Zero Vector**
```
0 = [0, 0, ..., 0]·µÄ
```

**2. Unit Vector** (length = 1)
```
||e|| = 1
Standard basis: e‚ÇÅ = [1,0,0], e‚ÇÇ = [0,1,0], e‚ÇÉ = [0,0,1]
```

**3. Sparse Vector** (mostly zeros)
- Common in NLP (word embeddings)
- Efficient storage

---

## üî¢ Vector Operations

### Addition
```
u + v = [u‚ÇÅ+v‚ÇÅ, u‚ÇÇ+v‚ÇÇ, ..., u‚Çô+v‚Çô]·µÄ
```

**Properties:**
- Commutative: u + v = v + u
- Associative: (u + v) + w = u + (v + w)

### Scalar Multiplication
```
Œ±v = [Œ±v‚ÇÅ, Œ±v‚ÇÇ, ..., Œ±v‚Çô]·µÄ
```

### Dot Product (Inner Product)
```
u ¬∑ v = u‚ÇÅv‚ÇÅ + u‚ÇÇv‚ÇÇ + ... + u‚Çôv‚Çô = Œ£·µ¢ u·µ¢v·µ¢
```

**Geometric Interpretation:**
```
u ¬∑ v = ||u|| ||v|| cos(Œ∏)
where Œ∏ is angle between vectors
```

**Properties:**
- Commutative: u ¬∑ v = v ¬∑ u
- Distributive: u ¬∑ (v + w) = u ¬∑ v + u ¬∑ w
- If u ¬∑ v = 0, vectors are orthogonal

### Vector Norm (Length)

**L2 Norm (Euclidean)**
```
||v||‚ÇÇ = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ... + v‚Çô¬≤) = ‚àö(v ¬∑ v)
```

**L1 Norm (Manhattan)**
```
||v||‚ÇÅ = |v‚ÇÅ| + |v‚ÇÇ| + ... + |v‚Çô|
```

**L‚àû Norm (Maximum)**
```
||v||‚àû = max(|v‚ÇÅ|, |v‚ÇÇ|, ..., |v‚Çô|)
```

**p-Norm (General)**
```
||v||‚Çö = (|v‚ÇÅ|·µñ + |v‚ÇÇ|·µñ + ... + |v‚Çô|·µñ)^(1/p)
```

### Unit Vector (Normalization)
```
vÃÇ = v / ||v||
```

---

## üìä Matrices

### Definition
A **matrix** is a rectangular array of numbers arranged in rows and columns.

**Notation:**
```
A = [a·µ¢‚±º]  where i = row, j = column

     [a‚ÇÅ‚ÇÅ  a‚ÇÅ‚ÇÇ  ...  a‚ÇÅ‚Çô]
A =  [a‚ÇÇ‚ÇÅ  a‚ÇÇ‚ÇÇ  ...  a‚ÇÇ‚Çô]
     [ ‚ãÆ    ‚ãÆ    ‚ã±    ‚ãÆ ]
     [a‚Çò‚ÇÅ  a‚Çò‚ÇÇ  ...  a‚Çò‚Çô]

Dimensions: m √ó n (m rows, n columns)
```

### Special Matrices

**1. Square Matrix** (m = n)
```
A is n √ó n
```

**2. Identity Matrix**
```
     [1  0  0]
I =  [0  1  0]
     [0  0  1]

AI = IA = A
```

**3. Diagonal Matrix**
```
     [d‚ÇÅ  0   0 ]
D =  [0   d‚ÇÇ  0 ]
     [0   0   d‚ÇÉ]

All off-diagonal elements = 0
```

**4. Zero Matrix**
```
All elements = 0
```

**5. Symmetric Matrix**
```
A = A·µÄ  (a·µ¢‚±º = a‚±º·µ¢)
```

**6. Orthogonal Matrix**
```
Q·µÄQ = QQ·µÄ = I
Columns are orthonormal vectors
```

**7. Triangular Matrices**
```
Upper Triangular: a·µ¢‚±º = 0 for i > j
Lower Triangular: a·µ¢‚±º = 0 for i < j
```

---

## üîß Matrix Operations

### Addition
```
C = A + B
c·µ¢‚±º = a·µ¢‚±º + b·µ¢‚±º

Requires: Same dimensions
```

### Scalar Multiplication
```
B = Œ±A
b·µ¢‚±º = Œ±¬∑a·µ¢‚±º
```

### Transpose
```
(A·µÄ)·µ¢‚±º = A‚±º·µ¢

If A is m√ón, then A·µÄ is n√óm
```

**Properties:**
- (A·µÄ)·µÄ = A
- (A + B)·µÄ = A·µÄ + B·µÄ
- (AB)·µÄ = B·µÄA·µÄ
- (Œ±A)·µÄ = Œ±A·µÄ

### Matrix-Vector Multiplication
```
y = Ax

y·µ¢ = Œ£‚±º a·µ¢‚±ºx‚±º

If A is m√ón and x is n√ó1, then y is m√ó1
```

**Interpretation:**
- Linear transformation
- Combination of column vectors

### Matrix-Matrix Multiplication
```
C = AB

c·µ¢‚±º = Œ£‚Çñ a·µ¢‚Çñb‚Çñ‚±º

If A is m√ón and B is n√óp, then C is m√óp
```

**Properties:**
- NOT commutative: AB ‚â† BA (in general)
- Associative: (AB)C = A(BC)
- Distributive: A(B+C) = AB + AC

---

## üéØ AI/ML Applications

### 1. Data Representation
```python
# Dataset as matrix
X = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]·µÄ  # n samples
Each x·µ¢ is d-dimensional feature vector

X is n √ó d matrix
```

### 2. Linear Transformations
```python
# Neural network layer
y = Wx + b

W: weight matrix
x: input vector
b: bias vector
y: output vector
```

### 3. Image Representation
```python
# Grayscale image: 2D matrix
# RGB image: 3D tensor (H √ó W √ó 3)
# Batch of images: 4D tensor (N √ó H √ó W √ó 3)
```

### 4. Word Embeddings
```python
# Embedding matrix
E: V √ó d  (V = vocabulary size, d = embedding dim)
Each row = word vector
```

---

## ‚ö° Computational Complexity

### Operation Complexity

| Operation | Time Complexity | Space | Notes |
|-----------|----------------|-------|-------|
| Vector addition | O(n) | O(n) | Element-wise |
| Dot product | O(n) | O(1) | Single pass |
| Vector norm | O(n) | O(1) | Sum + sqrt |
| Matrix-vector mult | O(mn) | O(m) | m√ón matrix |
| Matrix-matrix mult | O(mnp) | O(mp) | m√ón √ó n√óp |
| Transpose | O(1) | O(1) | View only (NumPy) |

### Scalability Considerations

**Small Dimensions (n < 1000):**
- Use dense matrices
- Standard NumPy operations
- No special optimization needed

**Medium Dimensions (1000 < n < 100,000):**
- Consider sparse matrices if >90% zeros
- Use optimized BLAS libraries
- Batch operations when possible

**Large Dimensions (n > 100,000):**
- **Must use sparse matrices**
- Distributed computing (Dask, Ray)
- Approximate methods (randomized algorithms)

---

## üõ°Ô∏è Numerical Stability

### Critical Issues in Production

**1. Overflow/Underflow**
```python
# BAD: Can overflow
exp_values = np.exp(large_numbers)

# GOOD: Subtract max for stability
exp_values = np.exp(large_numbers - np.max(large_numbers))
```

**2. Loss of Precision**
```python
# BAD: Catastrophic cancellation
result = (a + b) - (a + c)  # If a >> b,c

# GOOD: Rearrange
result = b - c
```

**3. Ill-Conditioned Matrices**
```python
# Check condition number before inversion
cond = np.linalg.cond(A)
if cond > 1e10:
    print(f"Warning: Matrix is ill-conditioned (Œ∫={cond:.2e})")
    # Use regularization or pseudo-inverse
    A_reg = A + 1e-6 * np.eye(A.shape[0])
```

**4. Normalization Issues**
```python
# BAD: Division by zero
normalized = v / np.linalg.norm(v)

# GOOD: Add epsilon
normalized = v / (np.linalg.norm(v) + 1e-8)
```

### Production Best Practices

‚úÖ **Always check for:**
- NaN values: `np.isnan(A).any()`
- Inf values: `np.isinf(A).any()`
- Condition number: `np.linalg.cond(A)`
- Rank deficiency: `np.linalg.matrix_rank(A)`

‚úÖ **Use stable algorithms:**
- `np.linalg.solve()` instead of `np.linalg.inv()`
- `np.linalg.lstsq()` for overdetermined systems
- SVD for rank-deficient matrices

---

## üéì Advanced Topics

### Linear Independence

**Definition:** Vectors v‚ÇÅ, v‚ÇÇ, ..., v‚Çô are **linearly independent** if:
```
c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çôv‚Çô = 0  ‚üπ  c‚ÇÅ = c‚ÇÇ = ... = c‚Çô = 0
```

**Check in NumPy:**
```python
def are_linearly_independent(vectors):
    """Check if column vectors are linearly independent"""
    A = np.column_stack(vectors)
    rank = np.linalg.matrix_rank(A)
    return rank == len(vectors)

# Example
v1 = np.array([1, 0, 0])
v2 = np.array([0, 1, 0])
v3 = np.array([1, 1, 0])  # Linear combination of v1, v2

print(are_linearly_independent([v1, v2]))  # True
print(are_linearly_independent([v1, v2, v3]))  # True (still independent)
```

### Span and Basis

**Span:** Set of all linear combinations
```
span(v‚ÇÅ, v‚ÇÇ, ..., v‚Çô) = {c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çôv‚Çô : c·µ¢ ‚àà ‚Ñù}
```

**Basis:** Linearly independent set that spans the space
```python
# Standard basis for ‚Ñù¬≥
e1 = np.array([1, 0, 0])
e2 = np.array([0, 1, 0])
e3 = np.array([0, 0, 1])

# Any vector in ‚Ñù¬≥ can be written as combination
v = 2*e1 + 3*e2 + 4*e3  # [2, 3, 4]
```

### Vector Spaces

**Column Space (Range):** Span of column vectors
```python
def column_space_basis(A):
    """Find basis for column space"""
    Q, R = np.linalg.qr(A)
    rank = np.linalg.matrix_rank(A)
    return Q[:, :rank]
```

**Null Space (Kernel):** Solutions to Ax = 0
```python
def null_space(A, tol=1e-10):
    """Find basis for null space"""
    U, s, Vt = np.linalg.svd(A)
    null_mask = s < tol
    return Vt[null_mask].T
```

### Outer Product

**Definition:** u ‚äó v = uv·µÄ (rank-1 matrix)
```python
u = np.array([1, 2, 3])
v = np.array([4, 5])

# Outer product
A = np.outer(u, v)
print(A.shape)  # (3, 2)
print(np.linalg.matrix_rank(A))  # 1 (rank-1 matrix)
```

**Application:** Building matrices from vectors
```python
# Covariance matrix from centered data
X_centered = X - X.mean(axis=0)
Cov = (X_centered.T @ X_centered) / (len(X) - 1)
# Each term is an outer product!
```

---

## üíª Practical Workflows

### NumPy Implementation

```python
import numpy as np

# Create vectors
v = np.array([1, 2, 3])
u = np.array([4, 5, 6])

# Vector operations
v_plus_u = v + u
v_scaled = 2 * v
dot_product = np.dot(v, u)  # or v @ u
norm = np.linalg.norm(v)  # L2 norm
unit_vector = v / norm

# Create matrices
A = np.array([[1, 2], [3, 4], [5, 6]])  # 3√ó2
B = np.array([[7, 8, 9], [10, 11, 12]])  # 2√ó3

# Matrix operations
A_transpose = A.T
C = A @ B  # Matrix multiplication (3√ó3)
identity = np.eye(3)  # 3√ó3 identity

# Special matrices
zeros = np.zeros((3, 4))
ones = np.ones((2, 3))
diagonal = np.diag([1, 2, 3])

# Matrix-vector multiplication
x = np.array([1, 2])
y = A @ x  # Result: 3√ó1

# Element-wise operations
A_squared = A ** 2  # Element-wise square
A_times_2 = A * 2   # Element-wise multiplication
```

### Common Patterns

**1. Batch Processing**
```python
# Process multiple samples at once
X = np.random.randn(100, 784)  # 100 samples, 784 features
W = np.random.randn(784, 10)   # Weights
Y = X @ W  # (100, 10) - all samples processed together
```

**2. Broadcasting**
```python
# Add bias to all samples
X = np.random.randn(100, 10)
b = np.random.randn(10)
Y = X + b  # b is broadcast to (100, 10)
```

**3. Normalization**
```python
# Normalize each feature
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_normalized = (X - X_mean) / X_std
```

---

## üéì Interview Focus

### Key Questions

1. **What is the difference between a row vector and column vector?**
   - Row: 1√ón, Column: n√ó1
   - Transpose relationship
   - Different multiplication rules

2. **When can you multiply two matrices?**
   - A(m√ón) √ó B(n√óp) = C(m√óp)
   - Inner dimensions must match

3. **What does the dot product represent geometrically?**
   - Projection of one vector onto another
   - Measures similarity/alignment
   - Zero if orthogonal

4. **Why is matrix multiplication not commutative?**
   - AB ‚â† BA in general
   - Dimensions may not even match
   - Represents different transformations

5. **What is a symmetric matrix and why is it important?**
   - A = A·µÄ
   - Real eigenvalues
   - Common in covariance matrices

### Must-Know Formulas

```
Dot product: u ¬∑ v = Œ£·µ¢ u·µ¢v·µ¢
L2 norm: ||v|| = ‚àö(Œ£·µ¢ v·µ¢¬≤)
Matrix mult: (AB)·µ¢‚±º = Œ£‚Çñ a·µ¢‚Çñb‚Çñ‚±º
Transpose: (AB)·µÄ = B·µÄA·µÄ
```

### Common Pitfalls

- ‚ùå Forgetting dimension compatibility
- ‚ùå Confusing element-wise and matrix multiplication
- ‚ùå Not checking for square matrices when needed
- ‚ùå Assuming commutativity

---

## üîó Connections

### Prerequisites
- Basic algebra
- Coordinate systems

### Related Topics
- [Matrix Operations](Matrix-Operations.md)
- [Eigenvalues and Eigenvectors](Eigenvalues-and-Eigenvectors.md)
- [Linear Transformations](../2_Calculus/Derivatives-and-Gradients.md)

### Applications in AI
- Neural network layers
- Data preprocessing
- Dimensionality reduction (PCA)
- Embeddings

---

## üìö References

- **Books:**
  - "Introduction to Linear Algebra" - Gilbert Strang
  - "Linear Algebra and Its Applications" - David Lay
  
- **Online:**
  - [3Blue1Brown: Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
  - [MIT OCW: Linear Algebra](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/)
  
- **Practice:**
  - NumPy documentation
  - Linear algebra exercises on Khan Academy

---

**Master vectors and matrices - they are the language of AI!**
